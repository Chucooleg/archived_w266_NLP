{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeunghoman/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm_test' from '/home/yeunghoman/w266/assignment/a4/rnnlm_test.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) RNNLM Inputs and Parameters\n",
    "\n",
    "### Answers for Part (a)\n",
    "You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
    "\n",
    "1. Assuming that `CellFunc` does not include the `Affine Layer After RNN Output` and `W_out` or `b_out` are included in the \"green\" cell. Cell equation is \n",
    "$h^{(i)} = tanh(concat(h^{(i-1)},x^{(i)})*W_{cell} + B_{cell})$.`V` should not matter here. The total number of parameters for $W_{cell} = 2H^2$, for $B_cell = H$, total $2H^2 + H$.\n",
    "\n",
    "2. $W_{in}$ of embedding layer has shape `(V by H)`, total $V*H$ parameters. $W_{out}$ of output layer has shape `(H by V)` and $B_{out}$ has shape `(V,)`, total $H*V + V$ parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Assuming embedding look up doesn't count as a mathematical `FLOP`. But for each single input word the look up takes `O(1)`. \n",
    "\n",
    "\n",
    "#### Within the RNN cell: \n",
    "\n",
    "`sum(FLOP) in MatMul = [2H + (2H-1)]*H  ---- this is O(H^2)`\n",
    "\n",
    "`sum(FLOP) to add Bias term = H  ---- this is O(H)`\n",
    "\n",
    "`sum(FLOP) to calculate tanh = H * 5  ---- this is O(H)`\n",
    "\n",
    "`----- Overall, the cell func is dominated by O(H^2)`\n",
    "\n",
    "\n",
    "#### After RNN output, we use this output to get V logits:\n",
    "\n",
    "`sum(FLOP) in MatMul = [H + (H-1)]*V  ---- this is O(HV)`\n",
    "\n",
    "`sum(FLOP) to add Bias Term = V  ---- this is O(V)`\n",
    "\n",
    "`----- Overall, this part is dominated by O(HV)`\n",
    "\n",
    "\n",
    "#### Calculate softmax :\n",
    "\n",
    "`sum(FLOP) to exponentiate every term = V ---- this is O(V)`\n",
    "\n",
    "`sum(FLOP) to calculate denominator = V-1 ---- this is O(V)`\n",
    "\n",
    "`sum(FLOP) to get softmax probability for a specific word = 1 ---- this is O(1)`\n",
    "\n",
    "`----- Overall, this part is dominated by O(V)`\n",
    "\n",
    "\n",
    "#### sum(FLOP) for a Single Target Word in a specific time step \n",
    "\n",
    "`= [2H + (2H-1)]*H + 6H + [H + (H-1)]*V + 2V + V - 1 + 1`\n",
    "\n",
    "`= 4H^2 + 5H + 2HV + 2V`\n",
    "\n",
    "`--ANSWER-- Overall, this is dominated by O(H^2 + HV)`\n",
    "\n",
    "\n",
    "#### sum(FLOP) for all Target Words in a specific time step, we simply add V-1 more divisions for the probabilities. Every flop up to the softmax denominator is shared\n",
    "`= 4H^2 + 5H + 2HV + 3V - 1`\n",
    "`--ANSWER-- Again, this is dominated by O(H^2 + HV)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "### Sampled Softmax: \n",
    "\n",
    "`Basic idea is that we substitude V for (k+1) for everything after RNN output. Assume the sampling algorithm uses the same k samples for every operation in the same batch.`\n",
    "\n",
    "#### After the RNN output, sampled softmax only calculate logits for the K samples.\n",
    "\n",
    "`sum(FLOP) in MatMul = [H + (H-1)]*(k+1)  ---- this is O(Hk)`\n",
    "\n",
    "`sum(FLOP) to add Bias Term = k+1  ---- this is O(k)`\n",
    "\n",
    "`----- Overall, this part is dominated by O(Hk)`\n",
    "\n",
    "#### Calculate softmax :\n",
    "\n",
    "`sum(FLOP) to exponentiate every term = k+1 ---- this is O(k)`\n",
    "\n",
    "`sum(FLOP) to calculate denominator = k ---- this is O(k)`\n",
    "\n",
    "`sum(FLOP) to get softmax probability for a specific word = 1 ---- this is O(1)`\n",
    "\n",
    "`----- Overall, this part is dominated by O(k)`\n",
    "\n",
    "#### A Single Target Word \n",
    "\n",
    "`= [2H + (2H-1)]*H + 6H + [H + (H-1)]*(k+1) + 2(k+1) + k + 1`\n",
    "\n",
    "`= 4H^2 + 5H + 2H(k+1) + 2(k+1)`\n",
    "\n",
    "`--ANSWER-- Overall, this is dominated by O(H^2 + Hk)`\n",
    "\n",
    "#### All Target Words. We use the same k samples and simply add V-1 more divisions for the probabilities. There may be a slight complication/option that the denominator need to be updated with the current target logit. Every flop up to the softmax denominator is shared.\n",
    "\n",
    "`= [2H + (2H-1)]*H + 6H + [H + (H-1)]*(k+1) + 2(k+1) + (k - 1 + V) + V`\n",
    "\n",
    "`= 4H^2 + 5H + 2H(k+1) + 2k + 2V `\n",
    "\n",
    "`--ANSWER-- Overall, this is dominated by O(H^2 + Hk + V)`\n",
    "\n",
    "\n",
    "\n",
    "### Hierarchical softmax: Basic idea is that we take the RNN output O and take a maximum of log(V) tree splits until we reach the bottom.\n",
    "\n",
    "`sum(FLOP) for each split = [H + (H-1)]*1 + 1  ---- this is O(H)`\n",
    "`sum(FLOP) for all splits down the tree = log(V)*(2H) ---- this is O(H*log(V))`\n",
    "\n",
    "#### A Single Target Word \n",
    "\n",
    "`= [2H + (2H-1)]*H + 6H + log(V)*(2H)`\n",
    "\n",
    "`= 4H^2 + 5H + log(V)*(2H)`\n",
    "\n",
    "`--ANSWER-- Overall, this is dominated by O(H^2 + H*log(V))`\n",
    "\n",
    "#### All Target Words. We walk down the tree V times.\n",
    "\n",
    "`= 4H^2 + 5H + log(V)*(2H)*V`\n",
    "\n",
    "`--ANSWER-- Overall, this is dominated by O(H^2 + V*H*log(V))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "\n",
    "`In training, we propagate both forward and backward. The recurrent layer would require largest number of forward flops and largest number of backward parameter updates.`\n",
    "\n",
    "#### Forward:\n",
    "`Embedding Layer Look up is O(1)`\n",
    "`Recurrent Layer sum(FLOP) is O(H^2), H^2 = 200*200`\n",
    "`Output Layer sum(FLOP) is O(Hk), Hk = 200*100`\n",
    "\n",
    "#### Backward: \n",
    "`Embedding Layer updates (only one input word vector) = 200`\n",
    "`Recurrent Layer updates = 400*200 + 200`\n",
    "`Output Layer updates (we substitute V by k=100) = 200*100 + 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/w266/a4_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a4\n",
    "tensorboard --logdir /tmp/w266/a4_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 1.469s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]  #w_1, w_2,.... time step in a batch\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.01):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.use_dropout_:use_dropout,\n",
    "                     lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.learning_rate_:learning_rate,\n",
    "                     lm.initial_h_:h}\n",
    "        \n",
    "        cost, h, _ = session.run([loss, lm.final_h_ ,train_op], feed_dict=feed_dict)\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.01, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 9]: seen 500 words at 498.5 wps, loss = 1.643\n",
      "[batch 115]: seen 5800 words at 2883.5 wps, loss = 0.892\n",
      "[batch 225]: seen 11300 words at 3747.7 wps, loss = 1.021\n",
      "[batch 335]: seen 16800 words at 4179.6 wps, loss = 1.060\n",
      "[batch 445]: seen 22300 words at 4440.4 wps, loss = 1.024\n",
      "[batch 556]: seen 27850 words at 4620.4 wps, loss = 0.948\n",
      "[batch 665]: seen 33300 words at 4731.6 wps, loss = 0.918\n",
      "[batch 775]: seen 38800 words at 4822.6 wps, loss = 0.895\n",
      "[batch 885]: seen 44300 words at 4894.0 wps, loss = 0.862\n",
      "[batch 995]: seen 49800 words at 4951.9 wps, loss = 0.849\n",
      "[batch 1104]: seen 55250 words at 4993.8 wps, loss = 0.852\n",
      "[batch 1217]: seen 60900 words at 5044.7 wps, loss = 0.838\n",
      "[batch 1332]: seen 66650 words at 5098.4 wps, loss = 0.827\n",
      "[batch 1443]: seen 72200 words at 5129.7 wps, loss = 0.831\n",
      "[batch 1552]: seen 77650 words at 5147.9 wps, loss = 0.835\n",
      "[batch 1661]: seen 83100 words at 5165.9 wps, loss = 0.837\n",
      "[batch 1770]: seen 88550 words at 5180.0 wps, loss = 0.841\n",
      "[batch 1877]: seen 93900 words at 5187.0 wps, loss = 0.841\n",
      "[batch 1986]: seen 99350 words at 5199.4 wps, loss = 0.845\n",
      "[batch 2096]: seen 104850 words at 5212.5 wps, loss = 0.841\n",
      "[batch 2205]: seen 110300 words at 5221.9 wps, loss = 0.847\n",
      "[batch 2314]: seen 115750 words at 5232.0 wps, loss = 0.844\n",
      "[batch 2422]: seen 121150 words at 5238.2 wps, loss = 0.840\n",
      "[batch 2525]: seen 126300 words at 5234.1 wps, loss = 0.842\n",
      "[batch 2627]: seen 131400 words at 5228.6 wps, loss = 0.839\n",
      "[batch 2734]: seen 136750 words at 5232.8 wps, loss = 0.847\n",
      "[batch 2839]: seen 142000 words at 5232.0 wps, loss = 0.848\n",
      "[batch 2942]: seen 147150 words at 5228.4 wps, loss = 0.849\n",
      "[batch 3045]: seen 152300 words at 5224.5 wps, loss = 0.852\n",
      "[batch 3153]: seen 157700 words at 5229.0 wps, loss = 0.852\n",
      "[batch 3264]: seen 163250 words at 5239.0 wps, loss = 0.845\n",
      "[batch 3370]: seen 168550 words at 5240.8 wps, loss = 0.841\n",
      "[batch 3477]: seen 173900 words at 5243.8 wps, loss = 0.835\n",
      "[batch 3582]: seen 179150 words at 5243.4 wps, loss = 0.831\n",
      "[batch 3689]: seen 184500 words at 5245.7 wps, loss = 0.828\n",
      "[batch 3798]: seen 189950 words at 5250.4 wps, loss = 0.827\n",
      "[batch 3905]: seen 195300 words at 5252.8 wps, loss = 0.825\n",
      "Train set: avg. loss: 0.049  (perplexity: 1.05)\n",
      "Test set: avg. loss: 0.081  (perplexity: 1.08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 40.039s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 99]: seen 5000 words at 4991.9 wps, loss = 0.956\n",
      "[batch 213]: seen 10700 words at 5324.3 wps, loss = 0.763\n",
      "[batch 323]: seen 16200 words at 5379.0 wps, loss = 0.773\n",
      "[batch 433]: seen 21700 words at 5397.8 wps, loss = 0.817\n",
      "[batch 540]: seen 27050 words at 5378.8 wps, loss = 0.835\n",
      "[batch 647]: seen 32400 words at 5369.5 wps, loss = 0.837\n",
      "[batch 755]: seen 37800 words at 5369.4 wps, loss = 0.842\n",
      "[batch 865]: seen 43300 words at 5383.3 wps, loss = 0.866\n",
      "[batch 973]: seen 48700 words at 5384.2 wps, loss = 0.883\n",
      "[batch 1087]: seen 54400 words at 5411.6 wps, loss = 0.902\n",
      "[batch 1197]: seen 59900 words at 5416.1 wps, loss = 0.891\n",
      "[batch 1306]: seen 65350 words at 5416.1 wps, loss = 0.878\n",
      "[batch 1416]: seen 70850 words at 5418.8 wps, loss = 0.865\n",
      "[batch 1525]: seen 76300 words at 5419.8 wps, loss = 0.854\n",
      "[batch 1632]: seen 81650 words at 5414.8 wps, loss = 0.837\n",
      "[batch 1741]: seen 87100 words at 5414.6 wps, loss = 0.823\n",
      "[batch 1849]: seen 92500 words at 5411.4 wps, loss = 0.833\n",
      "[batch 1956]: seen 97850 words at 5407.2 wps, loss = 0.831\n",
      "[batch 2065]: seen 103300 words at 5407.0 wps, loss = 0.829\n",
      "[batch 2174]: seen 108750 words at 5408.6 wps, loss = 0.823\n",
      "[batch 2284]: seen 114250 words at 5412.4 wps, loss = 0.813\n",
      "[batch 2394]: seen 119750 words at 5415.0 wps, loss = 0.801\n",
      "[batch 2503]: seen 125200 words at 5415.6 wps, loss = 0.802\n",
      "[batch 2613]: seen 130700 words at 5418.0 wps, loss = 0.804\n",
      "[batch 2722]: seen 136150 words at 5418.6 wps, loss = 0.802\n",
      "[batch 2833]: seen 141700 words at 5421.9 wps, loss = 0.799\n",
      "[batch 2943]: seen 147200 words at 5423.5 wps, loss = 0.800\n",
      "[batch 3055]: seen 152800 words at 5428.4 wps, loss = 0.805\n",
      "[batch 3167]: seen 158400 words at 5432.8 wps, loss = 0.806\n",
      "[batch 3279]: seen 164000 words at 5437.9 wps, loss = 0.807\n",
      "[batch 3397]: seen 169900 words at 5451.9 wps, loss = 0.811\n",
      "[batch 3508]: seen 175450 words at 5454.4 wps, loss = 0.807\n",
      "[batch 3617]: seen 180900 words at 5453.7 wps, loss = 0.807\n",
      "[batch 3727]: seen 186400 words at 5454.6 wps, loss = 0.806\n",
      "[batch 3837]: seen 191900 words at 5455.0 wps, loss = 0.808\n",
      "[batch 3943]: seen 197200 words at 5450.6 wps, loss = 0.808\n",
      "Train set: avg. loss: 0.069  (perplexity: 1.07)\n",
      "Test set: avg. loss: 0.070  (perplexity: 1.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 38.201s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a4_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a4_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/yeunghoman/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 10,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:02:32\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:02:31\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:02:27\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:02:28\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:02:31\n",
      "[epoch 5] Train set: avg. loss: 4.847  (perplexity: 127.37)\n",
      "[epoch 5] Test set: avg. loss: 4.982  (perplexity: 145.72)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a trainingtick_s epoch.\n",
    "        run_epoch(lm, session, bi, learning_rate=0.01, train=True, verbose=True, tick_s=3600)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        if epoch == num_epochs:\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing epochs to 20 (saved as final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:02:32\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:02:34\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:02:28\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:02:29\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:02:26\n",
      "[epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:02:28\n",
      "[epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:02:27\n",
      "[epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:02:31\n",
      "[epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:02:31\n",
      "[epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:02:27\n",
      "[epoch 11] Starting epoch 11\n",
      "[epoch 11] Completed in 0:02:35\n",
      "[epoch 12] Starting epoch 12\n",
      "[epoch 12] Completed in 0:02:34\n",
      "[epoch 13] Starting epoch 13\n",
      "[epoch 13] Completed in 0:02:31\n",
      "[epoch 14] Starting epoch 14\n",
      "[epoch 14] Completed in 0:02:32\n",
      "[epoch 15] Starting epoch 15\n",
      "[epoch 15] Completed in 0:02:26\n",
      "[epoch 16] Starting epoch 16\n",
      "[epoch 16] Completed in 0:02:26\n",
      "[epoch 17] Starting epoch 17\n",
      "[epoch 17] Completed in 0:02:29\n",
      "[epoch 18] Starting epoch 18\n",
      "[epoch 18] Completed in 0:02:30\n",
      "[epoch 19] Starting epoch 19\n",
      "[epoch 19] Completed in 0:02:27\n",
      "[epoch 20] Starting epoch 20\n",
      "[epoch 20] Completed in 0:02:28\n",
      "[epoch 20] Train set: avg. loss: 4.631  (perplexity: 102.66)\n",
      "[epoch 20] Test set: avg. loss: 4.878  (perplexity: 131.41)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a trainingtick_s epoch.\n",
    "        run_epoch(lm, session, bi, learning_rate=0.01, train=True, verbose=True, tick_s=3600)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        if epoch == num_epochs:\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition, increase hidden layer nodes to 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=150, \n",
    "                    softmax_ns=200, #k\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:00:47\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:00:46\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:00:46\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:00:46\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:00:46\n",
      "[epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:00:46\n",
      "[epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:00:46\n",
      "[epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:00:46\n",
      "[epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:00:46\n",
      "[epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:00:46\n",
      "[epoch 11] Starting epoch 11\n",
      "[epoch 11] Completed in 0:00:46\n",
      "[epoch 12] Starting epoch 12\n",
      "[epoch 12] Completed in 0:00:47\n",
      "[epoch 13] Starting epoch 13\n",
      "[epoch 13] Completed in 0:00:47\n",
      "[epoch 14] Starting epoch 14\n",
      "[epoch 14] Completed in 0:00:46\n",
      "[epoch 15] Starting epoch 15\n",
      "[epoch 15] Completed in 0:00:46\n",
      "[epoch 16] Starting epoch 16\n",
      "[epoch 16] Completed in 0:00:46\n",
      "[epoch 17] Starting epoch 17\n",
      "[epoch 17] Completed in 0:00:46\n",
      "[epoch 18] Starting epoch 18\n",
      "[epoch 18] Completed in 0:00:46\n",
      "[epoch 19] Starting epoch 19\n",
      "[epoch 19] Completed in 0:00:46\n",
      "[epoch 20] Starting epoch 20\n",
      "[epoch 20] Completed in 0:00:46\n",
      "[epoch 20] Train set: avg. loss: 4.535  (perplexity: 93.24)\n",
      "[epoch 20] Test set: avg. loss: 4.888  (perplexity: 132.67)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a trainingtick_s epoch.\n",
    "        run_epoch(lm, session, bi, learning_rate=0.01, train=True, verbose=True, tick_s=3600)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        if epoch == num_epochs:\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition, increase stack up to 2 RNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=150, \n",
    "                    softmax_ns=200, #k\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:01:00\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:01:00\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:01:00\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:01:00\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:01:00\n",
      "[epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:01:00\n",
      "[epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:01:00\n",
      "[epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:01:00\n",
      "[epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:01:00\n",
      "[epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:01:00\n",
      "[epoch 11] Starting epoch 11\n",
      "[epoch 11] Completed in 0:01:00\n",
      "[epoch 12] Starting epoch 12\n",
      "[epoch 12] Completed in 0:01:00\n",
      "[epoch 13] Starting epoch 13\n",
      "[epoch 13] Completed in 0:01:00\n",
      "[epoch 14] Starting epoch 14\n",
      "[epoch 14] Completed in 0:01:00\n",
      "[epoch 15] Starting epoch 15\n",
      "[epoch 15] Completed in 0:01:00\n",
      "[epoch 16] Starting epoch 16\n",
      "[epoch 16] Completed in 0:01:00\n",
      "[epoch 17] Starting epoch 17\n",
      "[epoch 17] Completed in 0:01:00\n",
      "[epoch 18] Starting epoch 18\n",
      "[epoch 18] Completed in 0:01:00\n",
      "[epoch 19] Starting epoch 19\n",
      "[epoch 19] Completed in 0:01:00\n",
      "[epoch 20] Starting epoch 20\n",
      "[epoch 20] Completed in 0:01:00\n",
      "[epoch 20] Train set: avg. loss: 4.776  (perplexity: 118.65)\n",
      "[epoch 20] Test set: avg. loss: 5.001  (perplexity: 148.59)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a trainingtick_s epoch.\n",
    "        run_epoch(lm, session, bi, learning_rate=0.01, train=True, verbose=True, tick_s=3600)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        if epoch == num_epochs:\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition, Larger Vocabulary = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/yeunghoman/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Vocabulary: 20,000 types\n",
      "Loaded 57,340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45,872 sentences (924,077 tokens)\n",
      "Test set: 11,468 sentences (237,115 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 20000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=150, \n",
    "                    softmax_ns=200, #k\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a4_model\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[epoch 1] Completed in 0:01:11\n",
      "[epoch 2] Starting epoch 2\n",
      "[epoch 2] Completed in 0:01:10\n",
      "[epoch 3] Starting epoch 3\n",
      "[epoch 3] Completed in 0:01:10\n",
      "[epoch 4] Starting epoch 4\n",
      "[epoch 4] Completed in 0:01:10\n",
      "[epoch 5] Starting epoch 5\n",
      "[epoch 5] Completed in 0:01:09\n",
      "[epoch 6] Starting epoch 6\n",
      "[epoch 6] Completed in 0:01:10\n",
      "[epoch 7] Starting epoch 7\n",
      "[epoch 7] Completed in 0:01:10\n",
      "[epoch 8] Starting epoch 8\n",
      "[epoch 8] Completed in 0:01:10\n",
      "[epoch 9] Starting epoch 9\n",
      "[epoch 9] Completed in 0:01:11\n",
      "[epoch 10] Starting epoch 10\n",
      "[epoch 10] Completed in 0:01:10\n",
      "[epoch 11] Starting epoch 11\n",
      "[epoch 11] Completed in 0:01:11\n",
      "[epoch 12] Starting epoch 12\n",
      "[epoch 12] Completed in 0:01:11\n",
      "[epoch 13] Starting epoch 13\n",
      "[epoch 13] Completed in 0:01:11\n",
      "[epoch 14] Starting epoch 14\n",
      "[epoch 14] Completed in 0:01:11\n",
      "[epoch 15] Starting epoch 15\n",
      "[epoch 15] Completed in 0:01:10\n",
      "[epoch 16] Starting epoch 16\n",
      "[epoch 16] Completed in 0:01:10\n",
      "[epoch 17] Starting epoch 17\n",
      "[epoch 17] Completed in 0:01:10\n",
      "[epoch 18] Starting epoch 18\n",
      "[epoch 18] Completed in 0:01:10\n",
      "[epoch 19] Starting epoch 19\n",
      "[epoch 19] Completed in 0:01:10\n",
      "[epoch 20] Starting epoch 20\n",
      "[epoch 20] Completed in 0:01:10\n",
      "[epoch 20] Train set: avg. loss: 5.057  (perplexity: 157.18)\n",
      "[epoch 20] Test set: avg. loss: 5.385  (perplexity: 218.08)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a trainingtick_s epoch.\n",
    "        run_epoch(lm, session, bi, learning_rate=0.01, train=True, verbose=True, tick_s=3600)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        if epoch == num_epochs:\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "            print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "            score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "Tuning up number of epochs gave most perpplexity reduction. Increasing H (hidden state/embedding size) gives better in-sample fit but similar out of sample fit (sign of overfitting). Stacking another layer of LSTM revert perplexity back to original performance. Doubling vocabulary size gave the worst performance. Final model only increased size of epochs as it gives satisfactory in sample loss and lowest out of sample loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    feed_dict = {lm.input_w_:input_w, lm.initial_h_:initial_h, lm.use_dropout_:False}\n",
    "    final_h, samples = session.run([lm.final_h_, lm.pred_samples_], feed_dict=feed_dict)\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a4_model/rnnlm_trained\n",
      "<s> the president between DGDG , of these circumstance . <s> \n",
      "<s> competent , a measure is a line . <s> \n",
      "<s> electric <unk> men . <s> \n",
      "<s> to organize states , but `` in meaning of sharing its role '' . <s> \n",
      "<s> <unk> <unk> surgeon to the composite issue on stage . <s> \n",
      "<s> the <unk> of civilization . <s> \n",
      "<s> the reckless customs in the <unk> of hospitals and board has around a pulse grounds of food <unk> <unk> many \n",
      "<s> shayne thought to nor even it brought state runs on his sense . <s> \n",
      "<s> for chemistry , see the <unk> island of the convincing <unk> army department . <s> \n",
      "<s> the strains of the linguist . <s> \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a4_model/rnnlm_trained\n",
      "\"once upon a time\" : -9.34\n",
      "\"the quick brown fox jumps over the lazy dog\" : -8.44\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a4_model/rnnlm_trained\n",
      "\"the boy and the girl is\" : -6.59\n",
      "\"the boy and the girl are\" : -6.76\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl is\",\n",
    "         \"the boy and the girl are\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 1. question(s)\n",
    "\n",
    "Model thinks that `is` is more plausible. \n",
    "\n",
    "If the model doesn't order them correctly, it is likely because the LSTM hidden state \"forgot\" about `the boy` by the time we are using `girl` to predict the next work. The trainable weights in the forget gate haven't converged to a good enough final solution. Higher number of epochs and more training examples may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a4_model/rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -8.51\n",
      "\"peanuts are my favorite kind of vegetable\" : -7.97\n",
      "\"when I'm hungry I really prefer to eat\" : -8.23\n",
      "\"when I'm hungry I really prefer to drink\" : -8.65\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\",\n",
    "         \"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 2. question(s)\n",
    "\n",
    "Model thinks that `vegetable` is more likely than `nut`, and `eat` is more likely than `drink`. Note that `peanut` is not a `nut` but technically a `legume`. Also, it is neither `vegetable` nor `fruit`. So it would be fair to say that the model would feel ambivalent even if it is well-trained.\n",
    "\n",
    "Neither 3 or 5-gram will work well for these examples. They don't retain long enough memory.\n",
    "`peanuts are(1) my(2) fav(3) kind(4) of(5) [nut/vegetable]`, peanuts is out of range.\n",
    "`hungry I(1) really(2) prefer(3) to(4) [eat/drink](5)` , `hungry` is out of range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/w266/a4_model/rnnlm_trained\n",
      "\"I have lots of plastic green square toys\" : -8.66\n",
      "\"I have lots of green plastic square toys\" : -8.72\n",
      "\"I have lots of green square plastic toys\" : -8.77\n",
      "\"I have lots of plastic square green toys\" : -8.84\n",
      "\"I have lots of square plastic green toys\" : -8.85\n",
      "\"I have lots of square green plastic toys\" : -8.94\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to part 3. question(s)\n",
    "\n",
    "According to the image, `shape colour material` that is `square green plastic` should be the most natural order. But its log probability is also the lowest. In the training example, if most english sentences indeed adopt the suggested adjective order, then given enough examples, a tri-gram model may triumph by memory. On the other hand, with NN/LSTMs, the long term information captured are rather implicit, predictions are more meaning relevant than order precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
