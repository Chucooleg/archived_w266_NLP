{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Part-of-speech tagging with HMMs\n",
    "\n",
    "In this part of the assignment, we'll train a Hidden Markov Model (HMM) as a part-of-speech (POS) tagger, and implement both Forward-Backward inference and Viterbi decoding.\n",
    "\n",
    "In particular:\n",
    "- **(a)** Use the portion of the Penn Treebank available in the NLTK to estimate the transition and emission probabilities.\n",
    "- **(b)** Implement the Forward-Backward algorithm for marginal inference of $ P(y_i\\ |\\ x) $\n",
    "- **(c)** Implement the Viterbi algorithm to find the most likely tag _sequence_ $ \\hat{y} = \\arg\\max_{y'} P(y'\\ |\\ x) $\n",
    "\n",
    "**Note:** in the interest of a shorter assignment and giving you more time to work on projects, we've implemented **(a)** the training code and part of **(b)** the `backward()` algorithm. Do look over the solutions for those parts, as they're a good guide to the rest of your implementation!\n",
    "\n",
    "You may want to review the Week 7 and Week 8 async, as well as the slides on part-of-speech tagging which this assignment will follow closely.\n",
    "\n",
    "## Inspect NLTK/Penn Treebank\n",
    "Before continuing, let's take a few moments to inspect the format of the training data.  The Treebank Reader object has a `tagged_sents()` function that returns an list of sentences.  Each sentence consists of a list of (word, part of speech) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/yeunghoman/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "There are 3914 sentences in the corpus.\n",
      "The first sentence is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from importlib import reload\n",
    "\n",
    "from w266_common import utils\n",
    "import pos\n",
    "import pos_test\n",
    "import nltk\n",
    "\n",
    "# Use the sample of the Penn Treebank included with NLTK.\n",
    "assert(nltk.download('treebank'))\n",
    "corpus = nltk.corpus.treebank\n",
    "\n",
    "print('There are %d sentences in the corpus.' % len(corpus.tagged_sents()))\n",
    "print('The first sentence is:')\n",
    "corpus.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): HMM Parameterization\n",
    "\n",
    "![HMM diagram](HMM.png)\n",
    "\n",
    "Recall that a Hidden Markov Model models a sequence $[x_0, x_1, ..., x_n]$ as a Markov chain of _hidden_ states $[y_0, y_1, ..., y_n]$ and associated emissions $y_i \\to x_i$ at each position. Formally, we have three sets of parameters:\n",
    "\n",
    "1. An initial state probability $P(y_0)$ which determines the start state of the sequence.\n",
    "2. Transition probabilities $P(y_i\\ |\\ y_{i-1})$ from one state to the next.\n",
    "3. Emission probabilities $P(x_i\\ |\\ y_i)$ to generate an output at each timestep.\n",
    "\n",
    "For POS tagging, we treat the word (tokens) as the observed nodes $x_i$, and the part-of-speech tags as the hidden states $y_i$ associated with each token. At training time, since the data is fully tagged, we get to observe _both_ $x_i$ and $y_i$, and so we can train our model by maximum likelihood estimation.\n",
    "\n",
    "Recalling our n-gram models from Assignment 3, we can obtain the maximum likelihood parameters by simply counting. We'll use $t$ to denote a specific part-of-speech tag (from a tagset $T$), and $w$ to denote a specific word type (from a vocabulary $V$):\n",
    "\n",
    "1. Initial: $ P(y_0 = t) = \\frac{c(y_0 = t)}{\\sum_{t' \\in T} c(y_0 = t')} $\n",
    "2. Transition: $ P(y_i = t\\ |\\ y_{i-1} = t_1) = \\frac{c(t_1t)}{\\sum_{t' \\in T}c(t_1t')} $\n",
    "3. Emission: $ P(x_i = w\\ |\\ y_i = t) = \\frac{c(w\\ |\\ t)}{\\sum_{w' \\in V} c(w'\\ |\\ t)} $\n",
    "\n",
    "### Questions\n",
    "1.  Open `pos.py` and read `estimate_probabilities()` and `compute_logprobs()`.  This code should be very familiar as it's nearly identical to what you implemented in A3 to build an n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_estimate_probabilities (pos_test.TestTagging) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(pos)\n",
    "utils.run_tests(pos_test, [\"TestTagging.test_estimate_probabilities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Forward-Backward\n",
    "\n",
    "The forward-backward algorithm allows us to efficiently compute the most likely tag for any (or every) individual word. Formally, at each position $i$ we want to compute the marginal distribution $p(y_i\\ |\\ x_0, x_1, x_2, \\cdots, x_n) = p(y_i\\ |\\ x)$. Note that taking the most likely tag from this will not necessarily find the most likely _sequence_ of tags - we'll tackle that in part(c) with Viterbi.\n",
    "\n",
    "\n",
    "\n",
    "In [Week 7](https://docs.google.com/presentation/d/1DqZi3qLMlDxA-NDXiBRh34pPz2JK9hLYNcTNxDe39pk/edit#slide=id.g1eb138b3b3_1_72), we found that by applying Bayes rule and decomposing (for dynamic programming speedup), $p(y_i\\ |\\ x)$ can be computed with the following equations. Let $n$ be the length of the sentence, and $w_i$ be the (fixed) token at position $i = 0, 1, ..., n$:\n",
    "\n",
    "- $ \\alpha(0, t) = p(t) \\times p(x_0\\ |\\ t) $\n",
    "- $ \\alpha(i, t) = p(w_i\\ |\\ t) \\times \\sum_{t'} p(t\\ |\\ t') \\times \\alpha(i - 1, t') $\n",
    "- $ \\beta(n, t) = 1 $\n",
    "- $ \\beta(i-1, t) = \\sum_{t'} p(w_{i}\\ |\\ t') \\times p(t'\\ |\\ t) \\times \\beta(i, t') $\n",
    "\n",
    "Intuitively,\n",
    "- **Forward beliefs** $\\alpha(i, t)$ represent the sum of all the paths that end in tag $t$ at position $i$.\n",
    "- **Backward beliefs** $\\beta(i, t)$ represent the sum of all the ways to continue on from tag $t$ at position $i$ through to position $n$.\n",
    "\n",
    "If we combine the forward beliefs (information from before position $i$) with the backward beliefs (information from beyond position $i$), we get the exact probability distribution:\n",
    "\n",
    "$$ p(y_i = t\\ |\\ x) = \\alpha(i,t) \\times \\beta(i,t) $$\n",
    "\n",
    "### Log-probabilities\n",
    "\n",
    "Note that we're multiplying a lot of probabilities together in the above equations. While each term is easy to represent as a float, we can quickly run into numerical underflow issues when multiplying together a large number of small values.  If your dataset is as small as this and the sentences you want to tag are short (i.e. not work with real data), you can sometimes get away without worrying about this.  Alternatively, you can petition Intel to improve precision of floating point numbers close to 0.\n",
    "\n",
    "To avoid this, we'll perform all our calculations in log space. This means that we start with the log-probabilities (as computed by `HMM.compute_logprobs()`), and replace multiplication with addition and addition with log-sum-exp, according to the identities:\n",
    "\n",
    "- $ \\log(a b) = \\log(a) + \\log(b) $\n",
    "- $ \\log(a + b) = \\log(e^{\\log(a)} + e^{\\log(b)}) = \\text{LogSumExp}(log(a), log(b)) $\n",
    "\n",
    "To implement the latter, we recommend using [`scipy.misc.logsumexp`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.logsumexp.html), which is imported for you in the starter code.\n",
    "\n",
    "#### Cheat Sheet:  Summing probabilities\n",
    "\n",
    "Add probabilities together, $P(t_1) + P(t_2) + \\cdots + P(t_n)$.\n",
    "```python\n",
    "# \"Regular\" Probabilities\n",
    "sum_py = sum([p[t] for t in tags])\n",
    "# Log-probabilities\n",
    "log_sum_py = logsumexp([logp[t] for t in tags])\n",
    "```\n",
    "\n",
    "At the end of running this code,\n",
    "- `sum_py` $ = \\Sigma P(t_i)$\n",
    "- `log_sum_py` $ = log(\\Sigma P(t_i))$\n",
    "\n",
    "Normal and log-probabilities can always be converted into each other with a $e^x$ or $log(x)$, although you shouldn't need to do this explicitly in this assignment.\n",
    "\n",
    "_**Hint:**_ Your code in this part should look a lot like the math. In particular, `self.initial(...)`, `self.transition(...)`, and `self.emission(...)` are functions that are already set up to return appropriate defaults ($\\log p(...) = \\log 0 = -\\infty$) for unseen tags - so you shouldn't need to check membership with `if` statements or `dict.get()`.\n",
    "\n",
    "### Questions\n",
    "1.  Implement `forward()` (i.e. compute $\\alpha$) in `pos.py`\n",
    "2.  Inspect the code for `forward_backward()` in `pos.py`.\n",
    "3.  What does forward/backward do at a high level?\n",
    "4.  How does this manifest in the equations above?\n",
    "5.  What can you say about the sequence of tags it produces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers to Forward-Backward questions\n",
    "(Your answers to Q3-5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 3).\n",
    "`To make a best guess on the PoS tag of a particular position in a sentence, we consider a set of tag candidates. For each tag candidate, forward-backward sums up sequence probabilities from all sequences that contain the tag candidate at the position of interest. It then compares this score across all tag candidates and pick the tag with the highest score. `\n",
    "\n",
    "`The actual implementation is less direct than the overall idea sounds. For each tag candidate t at position i, the algorithm first split the sentence at position i. Then, the algorithm looks up both forward believe up to and including position i with tag t and backward believe from position i with tag t to the end. The product of forward and backward believes then gives us the sum of sequence probabilities from all sequences that contain the tag candidate t at position i.`\n",
    "\n",
    "\n",
    "\n",
    "#### Answer 4).\n",
    "`For each tag candidate t at position i, forward believe alpha(i,t) gives the total of sequence probabilities (including transmission and emission) that leads up to and including position i with tag t, while beta(i,t) gives the total of sequence probabilities that carries from position i with tag t to the end. alpha(0,t_0) concerns the start of sequence thus is initiated by product of probability of the tag t (rather than a transmission probability) and emission probability of word w_0 given t_0. beta(n,t_n) concerns the end of sequence thus is initiated by 1 -- there is only one way to carry forward from the end. Finally, the probability distribution of the possible tag labels at position i is derived from computing P(y_i=t|x)  for each t in the tag set, that is, the product of corresponding forward believe alpha(i,t) and backward believe beta(i,t) for each t.`\n",
    "\n",
    "\n",
    "\n",
    "#### Answer 5).\n",
    "`The sequence of tags it produces may not be coherent/ makes most grammar sense because individual tags are optimized at each position. This sequence probability can be lower than the best sequence probability from a more coherent/English like tag sequence.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_alpha (pos_test.TestTagging) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(pos)\n",
    "utils.run_tests(pos_test, [\"TestTagging.test_alpha\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_beta (pos_test.TestTagging) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(pos)\n",
    "utils.run_tests(pos_test, [\"TestTagging.test_beta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_backward (pos_test.TestTagging) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(pos)\n",
    "utils.run_tests(pos_test, [\"TestTagging.test_forward_backward\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's give it a try!\n",
    "\n",
    "(Warning: if you decide to try some of your own - and you should! - you may find the limited vocabulary of the training set to be problematic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(pos)\n",
    "hmm = pos.HMM()\n",
    "for sentence in corpus.tagged_sents():\n",
    "    hmm.update_counts(sentence)\n",
    "hmm.compute_logprobs()\n",
    "def pretty_print_fb(sentence):\n",
    "    print(sentence)\n",
    "    print(hmm.forward_backward(sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre will join the board .\n",
      "['NNP', 'MD', 'VB', 'DT', 'NN', '.']\n",
      "Pierre joined an organization .\n",
      "['NNP', 'VBD', 'DT', 'NN', '.']\n",
      "The old man .\n",
      "['DT', 'JJ', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_fb('Pierre will join the board .')\n",
    "pretty_print_fb('Pierre joined an organization .')\n",
    "pretty_print_fb('The old man .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the apple\n",
      "['DT', 'NN']\n",
      "Pierre ate the apple\n",
      "['``', '``', '``', '``']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_fb('the apple')\n",
    "pretty_print_fb('Pierre ate the apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The horse raced pass the barn fell.\n",
      "['``', '``', '``', '``', '``', '``', '``']\n",
      "I ate an apple\n",
      "['``', '``', '``', '``']\n",
      "Parking permit a parking spot\n",
      "['``', '``', '``', '``', '``']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_fb('The horse raced pass the barn fell.')\n",
    "pretty_print_fb('I ate an apple')\n",
    "pretty_print_fb('Parking permit a parking spot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c): Viterbi\n",
    "\n",
    "Viterbi finds the maximum likelihood sequence of assignments, rather than considering a single assignment at a time.  Its implementation is a small tweak on the $\\alpha$ of Forward Backward.  In particular, instead of trying to find the _sum_ of all the possible ways to make a part of speech in a particular position, we try to find the _best_ way. Formally, we have:\n",
    "\n",
    "- $\\delta(0, t) = p(t) \\times P(x_0\\ |\\ t)$\n",
    "- $\\delta(i, t) = p(x_i\\ |\\ t) \\times \\max_{t'} \\left[\\delta(i - 1, t') \\times p(t\\ |\\ t')\\right]$\n",
    "\n",
    "_**Hint:**_ As in part (b), your code should look quite a lot like the math above.\n",
    "\n",
    "### Questions:\n",
    "1.  Read the `viterbi()` function at the bottom of `pos.py`.  It uses the `delta` table and backpointers to determine the most likely sequence of part of speech tags.\n",
    "2.  Implement the equations immediately above pos.py's `build_viterbi_delta()`.\n",
    "3.  What does Viterbi do differently in its algorithm than forward of forward/backward?\n",
    "4.  What does this mean for the tags it produces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers to Viterbi questions\n",
    "(Your answers to Q3 and Q4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answers to Q3 \n",
    "\n",
    "`Both algorithm pre-compute partial sequence probabilities. Viterbi computes from left to right and store them in a single detla table while forward/backward computes from both left to right and right to left. The left to right forward believes are stored in the alpha table and the right to left backward believes are stored in the beta table.`\n",
    "\n",
    "`At inference time, since Viterbi only concerns the single sequence with best probabilities, the algorithm uses the back-pointer table to track the best tag for every position. Since Forward/backward concerns for each position the tag that accumulates most probabilities to the left and right of it, the algorithm computes for each tag its sum of forward (to the left) and backward (to the right) believes from the alpha and beta tables. The tag with the highest sum is selected.`\n",
    "\n",
    "#### answers to Q4\n",
    "`The tags it produces for a sequence are likely to be coherent and comply with English grammar, because all tags come from the same best sequence.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_build_viterbi_delta (pos_test.TestTagging) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(pos)\n",
    "utils.run_tests(pos_test, [\"TestTagging.test_build_viterbi_delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_viterbi_end_to_end (pos_test.TestTagging) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(pos)\n",
    "utils.run_tests(pos_test, [\"TestTagging.test_viterbi_end_to_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's give it a try!\n",
    "\n",
    "(Warning: if you decide to try some of your own - and you should! - you may find the limited vocabulary of the training set to be problematic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3914/3914 [00:01<00:00, 2645.02it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.treebank\n",
    "# Uncomment below if you install the full Penn Treebank\n",
    "# corpus = nltk.corpus.ptb\n",
    "\n",
    "# Optional: set to true to get a nice progressbar during training.\n",
    "use_fancy_progressbar = True\n",
    "if use_fancy_progressbar:\n",
    "    utils.require_package(\"tqdm\")\n",
    "    from tqdm import tqdm as ProgressBar\n",
    "else:\n",
    "    ProgressBar = lambda x: x  # pass-through iterator\n",
    "\n",
    "reload(pos)\n",
    "hmm = pos.HMM()\n",
    "for sentence in ProgressBar(corpus.tagged_sents()):\n",
    "    hmm.update_counts(sentence)\n",
    "hmm.compute_logprobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre/NNP will/MD join/VB the/DT board/NN ./.\n",
      "Pierre/NNP joined/VBD an/DT organization/NN ./.\n",
      "The/DT old/JJ man/NN ./.\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_v(sentence):\n",
    "    tokens = sentence.split()\n",
    "    tags = hmm.viterbi(tokens)\n",
    "    print(\" \".join(\"%s/%s\" % (w,t) for (w,t) in zip(tokens, tags)))\n",
    "\n",
    "pretty_print_v('Pierre will join the board .')\n",
    "pretty_print_v('Pierre joined an organization .')\n",
    "pretty_print_v('The old man .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the apple\n",
      "['DT', 'NN']\n",
      "Pierre ate the apple\n",
      "['``', '``', '``', '``']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_fb('the apple')\n",
    "pretty_print_fb('Pierre ate the apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The horse raced pass the barn fell.\n",
      "['``', '``', '``', '``', '``', '``', '``']\n",
      "I ate an apple\n",
      "['``', '``', '``', '``']\n",
      "Parking permit a parking spot\n",
      "['``', '``', '``', '``', '``']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_fb('The horse raced pass the barn fell.')\n",
    "pretty_print_fb('I ate an apple')\n",
    "pretty_print_fb('Parking permit a parking spot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
