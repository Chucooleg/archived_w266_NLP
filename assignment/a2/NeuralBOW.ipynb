{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Bag-of-Words Model\n",
    "\n",
    "In this notebook, we'll move beyond linear classifiers and implement a neural network for our classification task. \n",
    "\n",
    "We'll also introduce the [TensorFlow Estimator API](https://www.tensorflow.org/extend/estimators), which provides a high-level interface similar to scikit-learn. This involves a few new concepts, such as the idea of a `model_fn` and an `input_fn`, but it greatly simplifies experiments and reduces the need to write tedious data-feeding code.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Part (d):** Model architecture\n",
    "- **Part (e):** Implementing the Neural BOW model\n",
    "- **Introduction to `tf.Estimator`**\n",
    "- **Part (f):** Training, evaluation, and tuning\n",
    "\n",
    "As with the first half of the assignment, exercised are interspersed throughout the notebook. In particular, Part (d) has 4 questions, Part (e) asks you to write code in `models.py`, and Part (f) has 4 questions plus one optional implementation exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.4\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from w266_common import patched_numpy_io\n",
    "# Code for this assignment\n",
    "import sst, models, models_test\n",
    "\n",
    "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
    "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
    "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (d): Model Architecture\n",
    "\n",
    "The neural bag-of-words classifier is one of the simplest neural models for text classification. It takes its name from the bag-of-words assumption common to linear models, in which the weights for each input word are summed to make a prediction. For our neural version, we'll instead sum the _vector representations_ of each word, and then add feed-forward (hidden) layers to make a deep network.\n",
    "\n",
    "Here's a diagram:\n",
    "\n",
    "![Neural Bag-of-Words Model](images/neural_bow.png)\n",
    "\n",
    "We'll use the following notation:\n",
    "- $w^{(i)} \\in \\mathbb{Z}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x \\in \\mathbb{R}^d$ for the fixed-length vector given by summing all the $x^{(i)}$ for an example\n",
    "- $h^{(j)}$ for the hidden state after the $j^{th}$ fully-connected layer\n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Summing vectors:** $x = \\sum_{i=1}^n x^{(i)}$\n",
    "- **Hidden layer(s):** $h^{(j)} = f(h^{(j-1)} W^{(j)} + b^{(j)})$ where $h^{(-1)} = x$ and $j = 0,1,\\ldots,J-1$\n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(h^{(final)} W_{out} + b_{out})$ where $h^{(final)} = h^{(J-1)}$ is the output of the last hidden layer.\n",
    "\n",
    "As per usual, we define the logits to be the argument of the softmax:\n",
    "\n",
    "$$ \\mathrm{logits} = h^{(final)}W_{out} + b_{out} $$\n",
    "\n",
    "We'll refer to the first part of this model (**Embedding layer**, **Summing vectors**, and **Hidden layer(s)**) as the **Encoder**: it has the role of encoding the input sequence into a fixed-length vector representation that we pass to the output layer.\n",
    "\n",
    "We'll also use these as shorthand for important dimensions:\n",
    "- `V`: the vocabulary size (equal to `ds.vocab.size`)\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `hidden_dims`: a list of dimensions for the output of each hidden layer (i.e. $\\mathrm{dim}(h^{(j)})$&nbsp;=&nbsp;`hidden_dims[j]`)\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d) Short Answer Questions\n",
    "\n",
    "Answer the following in the cell below. \n",
    "\n",
    "1. Let `embed_dim = d`, `hidden_dims = [h1, h2]`, and `num_classes = k`. In terms of these values and the vocabulary size `V`, write down the shapes of the following variables: $W_{embed}$, $W^{(0)}$, $b^{(0)}$, $W^{(1)}$, $b^{(1)}$, $W_{out}$, $b_{out}$. (*Hint: $W_{embed}$ has a row for each word in the vocabulary.*)\n",
    "<p>\n",
    "2. Using your answer to 1., how many parameters (matrix or vector elements) are in the embedding layer? How about in the hidden layers? And the output layer?  \n",
    "<p>\n",
    "<p>\n",
    "3. Recall that logistic regression can be thought of as a single-layer neural network. What should we set as the values of `embed_dim` and `hidden_dims` such that this model implements logistic regression?\n",
    "<p>\n",
    "4. Suppose that we have two examples, `[foo bar baz]` and `[baz bar foo]`. Will this model make the same predictions on these? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d) Answers\n",
    "<a id=\"answers_d1234\"></a>\n",
    "\n",
    "1. \n",
    "$W_{embed}$ = `(V by d)`, \n",
    "$W^{(0)}$ = `(d by h1)`,\n",
    "$b^{(0)}$ = `(1 by h1)`,\n",
    "$W^{(1)}$ = `(h1 by h2)`,\n",
    "$b^{(1)}$ = `(1 by h2)`,\n",
    "$W_{out}$ = `(h2 by k)`,\n",
    "$b_{out}$ = `(1 by k)`\n",
    "2. \n",
    " - In the embedding layer $W_{embed}$, there are `V*d` parameters.\n",
    " - To produce the first hidden layer representation $h^{(0)}$, our weight matrix needs `d*h1` parameters and our bias term needs `1*h1` parameters. To prodice the second hidden layer representation $h^{(1)}$, our weight matrix needs `h1*h2` parameters and our bias term needs `1*h2` parameters. In total the hidden layers require us to estimate`(d+1)*h1 + (h1+1)*h2` parameters.\n",
    " - To produce the outputs, the output layers weight matrix needs `h2*k` parameters and the bias term needs `1*k` parameters. In total the output layer needs `(h2+1)*k` parameters.\n",
    "3. `embed_dim` can be any real number `d`. We don't need any values in hidden_dims because there are no hidden layers. $W_{out}$ should have shape `(d,k)` and shape of $b_{out}$ remains the same.\n",
    "4. Yes, the model will make the same sentiment predictions (argmax of softmax output). This is because when we sum the embeddings for all the x_s, the summing operation doesn't take the order of tokens into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Minibatches\n",
    "\n",
    "Modern hardware (especially GPUs) performs most efficiently when processing a large amount of data in parallel. Because of this, we usually feed data to a neural network in batches - that is, running several examples at a time, in parallel. If each example is represented by a vector $x \\in \\mathbb{R}^d$, then we can feed in a batch of $m$ examples as a matrix $X \\in \\mathbb{R}^{m \\times d}$, where each row is an example. Note that if we write our matrix-vector products with the vector on the left, as in the equations above, the batch dimension carries through while the rows remain independent:\n",
    "\n",
    "$$ H = f(X W + b) $$\n",
    "\n",
    "is equivalent to computing in parallel $H_i = f(X_i W + b)$ for each $i = 0, \\ldots, m - 1$. Most TensorFlow operations are designed to handle batching seamlessly, so long as $bs$ = `batch_size` is the first dimension of the input data.\n",
    "\n",
    "### Padding Sequences\n",
    "\n",
    "Unlike the Naive Bayes classifier, which took long ($d = V \\approx 16,000$) sparse vectors as input, our neural network will operate directly on a _sequence_ of ids (as stored in `ds.train.ids`). This can be variable-length (depending on the length of the sequence), but we'll need to coerce it into a fixed-length vector for training.\n",
    "\n",
    "The easiest thing to do here is to pad the vectors with a dummy index, which we can zero-out inside our model. Consider the inputs:\n",
    "```\n",
    "[great movies] (2 tokens)\n",
    "[this is a terrible movie] (5 tokens)\n",
    "```\n",
    "We'll convert these to IDs, then pad with a dummy index `0` to get a 2 x 5 matrix:\n",
    "```\n",
    "[[144, 104,  0,   0,  0 ]\n",
    " [ 20,  10,  6, 937, 21]]\n",
    "```\n",
    "\n",
    "For SST, we'll arbitrarily choose to pad to length 40, and clip any examples longer than that. _(Recall from Part (a) that this will only clip fewer than 5% of the dataset.)_\n",
    "\n",
    "The `ds.as_padded_array` function is implemented for you, and will handle clipping and padding automatically. Note the second return value, `*_ns`: this is a vector containing the original (clipped) sequence lengths. We'll use this inside the model to mask the dummy indices so they don't bias our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST from data/sst/trainDevTestTrees_PTB.zip\n",
      "Training set:     8,544 trees\n",
      "Development set:  1,101 trees\n",
      "Test set:         2,210 trees\n",
      "Building vocabulary - 16,474 words\n",
      "Processing to phrases...  Done!\n",
      "Splits: train / dev / test : 98,794 / 13,142 / 26,052\n"
     ]
    }
   ],
   "source": [
    "import sst\n",
    "ds = sst.SSTDataset(V=20000).process(label_scheme=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "train_x, train_ns, train_y = ds.as_padded_array('train', max_len=max_len, root_only=True)\n",
    "dev_x,   dev_ns,   dev_y   = ds.as_padded_array('dev',   max_len=max_len, root_only=True)\n",
    "test_x,  test_ns,  test_y  = ds.as_padded_array('test',  max_len=max_len, root_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " [[   4  606   10 3416    9   26    4 2821 1263   11  108   63 5543   64\n",
      "     7   13   75   11  277    9   84    6 4243   69 3417   40 1869 2822\n",
      "     5 8181 1682 5544   48  846 8182    3    0    0    0    0]\n",
      " [   4 2823 1870 5545    8   63    4 3418    8    4 2441   64 5546   10\n",
      "    46  905   13    6 5547    8  680   67   29 3419 2113 5548 1030  847\n",
      "    11 5549  623    8 8183 5550   11 8184    3    0    0    0]\n",
      " [8185 5551 2114 8186    6 8187    8 1530   36    6  167  769 1264    5\n",
      "     6  167   34  296 8188    9    4   51   36   16    4  307 3420  345\n",
      "   624    4 1031    5 4244    5  447    8    4  273    3    0]]\n",
      "Original sequence lengths:  [36 37 39]\n",
      "Target labels:  [1 1 1]\n",
      "\n",
      "Padded:\n",
      " the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . <s> <s> <s> <s>\n",
      "Un-padded:\n",
      " the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\\n\", train_x[:3])\n",
    "print(\"Original sequence lengths: \", train_ns[:3])\n",
    "print(\"Target labels: \", train_y[:3])\n",
    "print(\"\")\n",
    "print(\"Padded:\\n\", \" \".join(ds.vocab.ids_to_words(train_x[0])))\n",
    "print(\"Un-padded:\\n\", \" \".join(ds.vocab.ids_to_words(train_x[0,:train_ns[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (e): Implementing the Neural BOW Model\n",
    "\n",
    "In order to better manage the model code, we'll implement our BOW model in `models.py`. In particular, you'll need to implement the following functions:\n",
    "\n",
    "- `embedding_layer(...)`: constructs an embedding layer\n",
    "- `BOW_encoder(...)`: constructs the encoder stack as described above\n",
    "- `softmax_output_layer(...)`: constructs a softmax output layer\n",
    "\n",
    "**Follow the instructions in the code (function docstrings and comments) carefully!**\n",
    "\n",
    "In particular, for unit tests to work, you shouldn't change (or add) any `tf.name_scope` or `tf.variable_scope` calls, and must name the variables exactly as documented. (Your model may work just fine, of course, but the test harness will throw all sorts of errors!)\n",
    "\n",
    "To aid debugging and readability, we've adopted a convention that TensorFlow tensors are represented by variables ending in an underscore, such as `W_embed_` or `train_op_`.\n",
    "\n",
    "**Before you start**, be sure to answer the short-answer questions in Part (d). (_We guarantee that this section will be **much** harder if you don't!_)\n",
    "\n",
    "You may find the following TensorFlow API functions useful:\n",
    "- [`tf.nn.embedding_lookup`](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/embedding_lookup)\n",
    "- [`tf.nn.sparse_softmax_cross_entropy_with_logits`](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)\n",
    "- [`tf.reduce_mean`](https://www.tensorflow.org/versions/master/api_docs/python/tf/reduce_mean) and [`tf.reduce_sum`](https://www.tensorflow.org/versions/master/api_docs/python/tf/reduce_sum)\n",
    "\n",
    "**Do your work in `models.py`.** When ready, run the cell below to run the unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_embedding_layer (models_test.TestLayerBuilders) ... ok\n",
      "test_softmax_output_layer (models_test.TestLayerBuilders) ... ok\n",
      "test_BOW_encoder (models_test.TestNeuralBOW) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.056s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(models)\n",
    "utils.run_tests(models_test, [\"TestLayerBuilders\", \"TestNeuralBOW\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network (the hard way)\n",
    "\n",
    "In Assignment 1, we trained our simple model with a home-spun training loop, setting up `feed_dict`-s and making \n",
    "calls to `session.run()`. For demonstration, let's do the same here.\n",
    "\n",
    "We've implemented a wrapper function, `models.classifier_model_fn`, which uses the functions you wrote in **Part (e)** to build a model graph. It takes as input `features` and `labels` which contain input and target tensors, as well as `model` and `params` which configure the model. \n",
    "\n",
    "**Exercise (not graded):** Read through the code for `classifier_model_fn()` in `models.py`. Where is the code you wrote in Part (e) called? Where is the loss function set up, and what loss is used? How is the optimizer set up, and what options are available? What types of predictions are returned in the `predictions` dict?\n",
    "\n",
    "Using this function directly, we can write a simple training loop similar to Assignment 1's `train_nn()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  800 examples, moving-average loss 0.67\n",
      "1,600 examples, moving-average loss 0.53\n",
      "2,400 examples, moving-average loss 0.46\n",
      "3,200 examples, moving-average loss 0.50\n",
      "4,000 examples, moving-average loss 0.61\n",
      "4,800 examples, moving-average loss 0.47\n",
      "5,600 examples, moving-average loss 0.46\n",
      "6,400 examples, moving-average loss 0.43\n",
      "Completed one epoch in 0:00:01\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "x, ns, y = train_x, train_ns, train_y\n",
    "batch_size = 32\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=ds.vocab.size, embed_dim=50, hidden_dims=[25], num_classes=len(ds.target_names),\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "model_fn = models.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network with tf.Estimator\n",
    "\n",
    "As you see above, there's a lot of boilerplate involved with training a model - we need to instantiate the graph, manage a TensorFlow session, and manually feed data for each batch. This can get tedious, especially as we add support for checkpointing, saving models, and tracking statistics during training. To streamline this process, we can use a high-level api like `tf.Estimator`.\n",
    "\n",
    "The Estimator API allows us to define custom models, then provides an `Estimator` object that exposes `train()`, `evaluate()`, and `predict()` functions in a similar interface as scikit-learn. Take a few minutes to skim through the main documentation:\n",
    "\n",
    "- [TensorFlow Estimator API](https://www.tensorflow.org/extend/estimators)\n",
    "- [Estimators in 'Effective TensorFlow'](https://github.com/vahidk/EffectiveTensorflow#tf_learn) (advanced)\n",
    "\n",
    "### Model Functions (model_fn)\n",
    "\n",
    "The Estimator API is a functional interface, built around the idea of a `model_fn`. A `model_fn` is just a function that follows a specific interface, and when called constructs a graph of TensorFlow variables and ops that constitutes your model. Here's an example of what one looks like:\n",
    "\n",
    "```python\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    x_ = features['x']\n",
    "    logits_ = my_network(x_, hidden_dims=params['hidden_dims'],\n",
    "                         foo=params['foo'], bar=params['bar'])\n",
    "    \n",
    "    predictions_dict = {\"max\": tf.argmax(logits_, 1)}\n",
    "    eval_metrics = {\"accuracy\": tf.metrics_accuracy(predictions_dict['max']}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    loss_ = my_loss_fn(logits_)\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)\n",
    "```\n",
    "You can read more about the arguments here: \n",
    "- [Constructing the model_fn](https://www.tensorflow.org/extend/estimators#constructing_the_model_fn)\n",
    "\n",
    "The Estimator API takes a pointer to this _function_, then calls it internally to instantiate your model in the appropriate context. This allows it to handle things like writing and restoring checkpoints automatically, as well as feeding data to the model during training and evaluation. \n",
    "\n",
    "### Input Functions (input_fn)\n",
    "\n",
    "Data feeding is handled by an `input_fn`, which takes the place of the placeholder variables and `feed_dict` we'd otherwise need. The `input_fn` is defined separately from the `model_fn`, and builds the part of the graph up to `features` and `labels`.\n",
    "\n",
    "We won't write our own `input_fn` in this assignment, but instead we can just use the existing `numpy_input_fn` implementation. This takes NumPy arrays as inputs, and creates an `input_fn` that will generate minibatches:\n",
    "\n",
    "```python\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=32, num_epochs=20, shuffle=True\n",
    "                 )\n",
    "```\n",
    "\n",
    "You can read more about `input_fn`-s here: \n",
    "- [Building Input Functions with tf.Estimator](https://www.tensorflow.org/get_started/input_fn)\n",
    "\n",
    "**Note:** for this assignment, we'll use a patched version of `tf.estimator.inputs.numpy_input_fn` included with this assignment. This version allows us to seed the random number generator so that training data is shuffled but deterministic.\n",
    "\n",
    "### Building an Estimator\n",
    "\n",
    "With a `model_fn` and an `input_fn` in hand, we can now build and train an Estimator with just a couple of lines:\n",
    "\n",
    "```python\n",
    "model_params = dict(...)   # passed as 'params' to the model_fn\n",
    "model = tf.estimator.Estimator(model_fn=my_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=\"/tmp/my_model_checkpoints\")\n",
    "model.train(input_fn=train_input_fn)\n",
    "```\n",
    "\n",
    "The last line will kick off a train loop, ingesting data until the `input_fn` runs dry (20 epochs, for the one above). We can then evaluate on labeled data by calling `model.evaluate(input_fn=...)`, and run inference on unlabeled data by calling `model.predict(input_fn=...)` with appropriate `input_fn`-s.\n",
    "\n",
    "_**Note:** You might be wondering why TensorFlow adds all this boilerplate on top of the actual model. It doesn't seem necessary for small-scale experiments like this assignment, but as soon as you scale up to models that take hours, days, or even weeks to train, having robust checkpoint management, live dashboards, and distributed data queues really starts to pay off!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (f): Training and Evaluation\n",
    "\n",
    "The cell below defines some model params and sets up a checkpoint directory for TensorBoard.\n",
    "\n",
    "Use the following default parameters to start, as given below in `model_params`:\n",
    "```python\n",
    "embed_dim = 50\n",
    "hidden_dims = [25]  # single hidden layer\n",
    "optimizer = 'adagrad'\n",
    "lr = 0.1  # learning rate\n",
    "beta = 0.01  # L2 regularization\n",
    "```\n",
    "\n",
    "**Note:** Due to a bug in TensorFlow, if you re-use the same checkpoint directory (even after deleting the contents) it will sometimes fail to write the event data for TensorBoard. To work around this, the code below creates a new checkpoint directory each time with a name derived from the timestamp. You may want to delete these after a few runs, since they can take up ~35MB each. To do so just run:\n",
    "\n",
    "```sh\n",
    "# On command line\n",
    "rm -rfv /tmp/tf_bow_sst_*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (16,474 words) written to '/tmp/tf_bow_sst_20180211-0524/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20180211-0524/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20180211-0524', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f85e261d978>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20180211-0524' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=ds.vocab.size, embed_dim=50, hidden_dims=[25], num_classes=len(ds.target_names),\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below to start training! If you run TensorBoard from the command line, you should see loss curves in the \"Scalars\" tab as training progresses. We've set it up to run an evaluation on the dev set every `train_params['eval_every']` epochs, and this should appear in the same tab as a blue line after a couple minutes.\n",
    "\n",
    "Using the default `model_params` above and the following training params, as given in `train_params` below:\n",
    "```python\n",
    "batch_size = 32\n",
    "total_epochs = 20\n",
    "eval_every = 2  # every 2 epochs, eval the dev set\n",
    "```\n",
    "Your model should train very quickly - 20 epochs in under two minutes on a single-core GCE instance.\n",
    "\n",
    "After 20 epochs, your loss curves should look something like this:\n",
    "\n",
    "![Loss curves](images/tensorboard_curves.png)\n",
    "\n",
    "Don't worry if they don't match exactly - colors may vary, and the red dot labeled \"eval\\_test\" won't appear until you run the evaluation cell below. There are also some other curves that you might see: \"global\\_step/sec\" is the number of minibatches per second that the model processes, and the \"enqueue\\_input/...\" plot has to do with the feeder queues that the Estimator API uses to stream data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.10304, step = 1\n",
      "INFO:tensorflow:global_step/sec: 97.6156\n",
      "INFO:tensorflow:loss = 0.591804, step = 101 (1.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.871\n",
      "INFO:tensorflow:loss = 0.56995, step = 201 (0.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.549\n",
      "INFO:tensorflow:loss = 0.467347, step = 301 (0.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.9221\n",
      "INFO:tensorflow:loss = 0.412307, step = 401 (1.021 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 433 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.37697.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:14\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-433\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:15\n",
      "INFO:tensorflow:Saving dict for global step 433: accuracy = 0.716743, cross_entropy_loss = 0.589964, global_step = 433, loss = 0.72532\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-433\n",
      "INFO:tensorflow:Saving checkpoints for 434 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.187939, step = 434\n",
      "INFO:tensorflow:global_step/sec: 81.7218\n",
      "INFO:tensorflow:loss = 0.241181, step = 534 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.075\n",
      "INFO:tensorflow:loss = 0.33393, step = 634 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.411\n",
      "INFO:tensorflow:loss = 0.325257, step = 734 (0.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.016\n",
      "INFO:tensorflow:loss = 0.225925, step = 834 (0.972 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 866 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.184974.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-866\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:22\n",
      "INFO:tensorflow:Saving dict for global step 866: accuracy = 0.75344, cross_entropy_loss = 0.577482, global_step = 866, loss = 0.712166\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-866\n",
      "INFO:tensorflow:Saving checkpoints for 867 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.158775, step = 867\n",
      "INFO:tensorflow:global_step/sec: 98.7171\n",
      "INFO:tensorflow:loss = 0.179848, step = 967 (1.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.885\n",
      "INFO:tensorflow:loss = 0.246038, step = 1067 (0.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.502\n",
      "INFO:tensorflow:loss = 0.177162, step = 1167 (0.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.2115\n",
      "INFO:tensorflow:loss = 0.132086, step = 1267 (1.008 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1299 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1097.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:27\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-1299\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:27\n",
      "INFO:tensorflow:Saving dict for global step 1299: accuracy = 0.751147, cross_entropy_loss = 0.66972, global_step = 1299, loss = 0.814589\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-1299\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.122394, step = 1300\n",
      "INFO:tensorflow:global_step/sec: 104.213\n",
      "INFO:tensorflow:loss = 0.14109, step = 1400 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.626\n",
      "INFO:tensorflow:loss = 0.161843, step = 1500 (0.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.958\n",
      "INFO:tensorflow:loss = 0.13644, step = 1600 (0.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.222\n",
      "INFO:tensorflow:loss = 0.130521, step = 1700 (0.891 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1732 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.103687.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:32\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-1732\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:33\n",
      "INFO:tensorflow:Saving dict for global step 1732: accuracy = 0.752294, cross_entropy_loss = 0.687566, global_step = 1732, loss = 0.834786\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-1732\n",
      "INFO:tensorflow:Saving checkpoints for 1733 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.115005, step = 1733\n",
      "INFO:tensorflow:global_step/sec: 100.501\n",
      "INFO:tensorflow:loss = 0.124808, step = 1833 (0.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.751\n",
      "INFO:tensorflow:loss = 0.145478, step = 1933 (0.947 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.886\n",
      "INFO:tensorflow:loss = 0.130216, step = 2033 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.603\n",
      "INFO:tensorflow:loss = 0.125862, step = 2133 (0.965 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2165 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.097705.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:38\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-2165\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:38\n",
      "INFO:tensorflow:Saving dict for global step 2165: accuracy = 0.75, cross_entropy_loss = 0.691185, global_step = 2165, loss = 0.838418\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-2165\n",
      "INFO:tensorflow:Saving checkpoints for 2166 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.11371, step = 2166\n",
      "INFO:tensorflow:global_step/sec: 99.4445\n",
      "INFO:tensorflow:loss = 0.119296, step = 2266 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.445\n",
      "INFO:tensorflow:loss = 0.134251, step = 2366 (0.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.585\n",
      "INFO:tensorflow:loss = 0.127195, step = 2466 (0.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.838\n",
      "INFO:tensorflow:loss = 0.123041, step = 2566 (0.936 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2598 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0955552.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:45\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-2598\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:45\n",
      "INFO:tensorflow:Saving dict for global step 2598: accuracy = 0.752294, cross_entropy_loss = 0.686607, global_step = 2598, loss = 0.833749\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-2598\n",
      "INFO:tensorflow:Saving checkpoints for 2599 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.11146, step = 2599\n",
      "INFO:tensorflow:global_step/sec: 106.236\n",
      "INFO:tensorflow:loss = 0.116842, step = 2699 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.653\n",
      "INFO:tensorflow:loss = 0.128582, step = 2799 (0.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.273\n",
      "INFO:tensorflow:loss = 0.124567, step = 2899 (0.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.811\n",
      "INFO:tensorflow:loss = 0.120874, step = 2999 (0.928 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3031 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0939148.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-3031\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:50\n",
      "INFO:tensorflow:Saving dict for global step 3031: accuracy = 0.75344, cross_entropy_loss = 0.681852, global_step = 3031, loss = 0.828951\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-3031\n",
      "INFO:tensorflow:Saving checkpoints for 3032 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.109869, step = 3032\n",
      "INFO:tensorflow:global_step/sec: 108.706\n",
      "INFO:tensorflow:loss = 0.115151, step = 3132 (0.923 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 114.556\n",
      "INFO:tensorflow:loss = 0.124997, step = 3232 (0.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.388\n",
      "INFO:tensorflow:loss = 0.122526, step = 3332 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.469\n",
      "INFO:tensorflow:loss = 0.11915, step = 3432 (0.966 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3464 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0926181.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:38:55\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-3464\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:38:55\n",
      "INFO:tensorflow:Saving dict for global step 3464: accuracy = 0.752294, cross_entropy_loss = 0.677709, global_step = 3464, loss = 0.824788\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-3464\n",
      "INFO:tensorflow:Saving checkpoints for 3465 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.108596, step = 3465\n",
      "INFO:tensorflow:global_step/sec: 99.2089\n",
      "INFO:tensorflow:loss = 0.113856, step = 3565 (1.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.9137\n",
      "INFO:tensorflow:loss = 0.122356, step = 3665 (1.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.886\n",
      "INFO:tensorflow:loss = 0.120879, step = 3765 (0.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.761\n",
      "INFO:tensorflow:loss = 0.117751, step = 3865 (0.963 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3897 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0915492.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:39:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-3897\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:39:01\n",
      "INFO:tensorflow:Saving dict for global step 3897: accuracy = 0.748853, cross_entropy_loss = 0.674125, global_step = 3897, loss = 0.821197\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-3897\n",
      "INFO:tensorflow:Saving checkpoints for 3898 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.107535, step = 3898\n",
      "INFO:tensorflow:global_step/sec: 99.0709\n",
      "INFO:tensorflow:loss = 0.112832, step = 3998 (1.013 sec)\n",
      "INFO:tensorflow:global_step/sec: 102.574\n",
      "INFO:tensorflow:loss = 0.120291, step = 4098 (0.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.363\n",
      "INFO:tensorflow:loss = 0.119514, step = 4198 (0.967 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.82\n",
      "INFO:tensorflow:loss = 0.116594, step = 4298 (0.963 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4330 into /tmp/tf_bow_sst_20180211-0524/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0906442.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:39:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-4330\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:39:07\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.752294, cross_entropy_loss = 0.67096, global_step = 4330, loss = 0.81803\n"
     ]
    }
   ],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=32, total_epochs=20, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part(f).1: Evaluating Your Model\n",
    "\n",
    "To evaluate on the test set, we just need to construct another `input_fn`, then call `model.evaluate`. \n",
    "\n",
    "**1.)** Fill in the cell below, and run it to compute accuracy on the test set. With the default parameters, you should get accuracy around 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-11-05:45:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-4330\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-05:45:07\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.7743, cross_entropy_loss = 0.61913, global_step = 4330, loss = 0.763519\n",
      "Accuracy on test set: 77.43%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.77429986,\n",
       " 'cross_entropy_loss': 0.61912978,\n",
       " 'global_step': 4330,\n",
       " 'loss': 0.76351869}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (f).1\n",
    "# replace with an input_fn, similar to dev_input_fn\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False)  \n",
    "\n",
    "\n",
    "eval_metrics =  model.evaluate(input_fn=test_input_fn, name=\"test\")  # replace with result of model.evaluate(...)\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the old-fashioned way, by calling `model.predict(...)` and working with the predicted labels directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-4330\n",
      "Accuracy on test set: 77.43%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f).2: Evaluating on \"Interesting\" examples\n",
    "\n",
    "Write your answer in the cell below.\n",
    "\n",
    "**Question 2.)** In the cell below, repeat what you did above, but evaluate the model on the \"interesting\" examples. Does the neural bag-of-words model perform well here, as compared to the test set as a whole? How about compared to the Naive Bayes baseline? Explain why this might be, in terms of the phenomena you found in Part (c).2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f).2 Answers\n",
    "<a id=\"answers_f2\"></a>\n",
    "\n",
    "**2.)** Compared to the test set as a whole, our model performs worse on the interesting examples. Mechanically, the naive bayes model simply sum up weights for all the word occurrences for the final prediction and the neural BOW model simply sums up (collapsing) word embeddings for the feed forward layers. Therefore, both models are throwing away any information of word order or structured subphrase sentiments, thus they don't work well when contrast words twists the overall sentiments of the sentences. Compared to the Naive Bayes baseline, the neural BOW model accuracy is a few percentage points lower. This is possibly because the number of parameters we are estimating is  `V*d + (d+1)*h1 + (h1+1)*k` which is much much higher than `V + 1` in the NB model. We only have 6920 examples in `train_x` so both our embeddings are weights are not well trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 246 interesting examples\n",
      "Interesting ids (into ds.test_trees):  [0, 27, 31, 32, 75, 80, 90, 96, 117, 124, 138, 140, 141, 160, 166, 186, 187, 205, 210, 212, 227, 232, 254, 269, 271, 285, 296, 307, 312, 327, 335, 373, 397, 399, 406, 407, 410, 426, 447, 511, 512, 516, 521, 534, 539, 563, 577, 588, 606, 610, 611, 637, 640, 645, 655, 662, 664, 713, 720, 721, 724, 739, 755, 758, 763, 776, 791, 793, 796, 802, 805, 810, 818, 840, 858, 887, 898, 899, 909, 910, 912, 929, 930, 961, 970, 973, 974, 975, 979, 1008, 1032, 1036, 1066, 1067, 1076, 1098, 1101, 1108, 1114, 1131, 1138, 1142, 1159, 1183, 1185, 1189, 1193, 1198, 1206, 1214, 1215, 1235, 1241, 1243, 1244, 1261, 1267, 1273, 1275, 1279, 1280, 1293, 1296, 1302, 1303, 1312, 1318, 1319, 1321, 1322, 1324, 1326, 1328, 1338, 1341, 1346, 1359, 1363, 1371, 1383, 1398, 1402, 1413, 1443, 1452, 1456, 1458, 1462, 1464, 1480, 1481, 1486, 1487, 1488, 1507, 1509, 1513, 1516, 1527, 1537, 1552, 1576, 1582, 1587, 1594, 1597, 1602, 1607, 1608, 1615, 1619, 1622, 1629, 1630, 1639, 1666, 1682, 1688, 1694, 1727, 1728, 1731, 1755, 1763, 1764, 1786, 1789, 1793, 1795, 1798, 1804, 1807, 1817, 1830, 1834, 1850, 1852, 1883, 1885, 1899, 1903, 1908, 1910, 1918, 1929, 1933, 1939, 1940, 1942, 1943, 1945, 1947, 1949, 1950, 1952, 1954, 1964, 1965, 1970, 1972, 1980, 1982, 2008, 2009, 2011, 2021, 2035, 2036, 2038, 2063, 2079, 2085, 2089, 2094, 2096, 2118, 2119, 2121, 2143, 2145, 2149, 2150, 2160, 2164, 2190, 2193]\n",
      "\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-06:05:49\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0524/model.ckpt-4330\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-06:05:49\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.715116, cross_entropy_loss = 0.812027, global_step = 4330, loss = 0.939525\n",
      "Accuracy on test set: 71.51%\n"
     ]
    }
   ],
   "source": [
    "df = ds.test\n",
    "\n",
    "gb = df.groupby(by=['root_id'])\n",
    "interesting_ids = []   # root ids, index into ds.test_trees\n",
    "interesting_idxs = []  # DataFrame indices, index into ds.test\n",
    "# This groups the DataFrame by sentence\n",
    "for root_id, idxs in gb.groups.items():\n",
    "    # Get the average score of all the phrases for this sentence\n",
    "    mean = df.loc[idxs].label.mean()\n",
    "    if (mean > 0.4 and mean < 0.6):\n",
    "        interesting_ids.append(root_id)\n",
    "        interesting_idxs.extend(idxs)\n",
    "        \n",
    "print(\"Found {:,} interesting examples\".format(len(interesting_ids)))\n",
    "print(\"Interesting ids (into ds.test_trees): \", interesting_ids)\n",
    "print(\"\")\n",
    "\n",
    "# This will extract only the \"interesting\" sentences we found above\n",
    "test_x_interesting, test_ns_interesting, test_y_interesting = ds.as_padded_array(\"test\", root_only=True, \n",
    "                                                                                 df_idxs=interesting_idxs)\n",
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (f).2\n",
    "test_input_fn_interesting = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x_interesting, \"ns\": test_ns_interesting}, y=test_y_interesting,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False)  \n",
    "\n",
    "\n",
    "eval_metrics_interesting =  model.evaluate(input_fn=test_input_fn_interesting, name=\"test\")\n",
    "\n",
    "acc = eval_metrics_interesting['accuracy']  # replace with actual value\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Tuning Your Model\n",
    "\n",
    "Our default model from Part (e) performs decently, but doesn't manage to beat even the Naive Bayes baseline. We might be able to fix that with a bit of tuning.\n",
    "\n",
    "Answer the following in the cell below.\n",
    "\n",
    "**Question 3.)** Look at your training curves in TensorBoard, after 20 epochs with the default parameters. Do you think that the model would benefit from more training time?\n",
    "<p>\n",
    "**Question 4.)** Based on the accuracy trace (on the dev set) and the cross entropy loss curves on the training and dev sets, do you think the model is overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers for Part (f).3 and 4\n",
    "<a id=\"answers_f34\"></a>\n",
    "\n",
    "**3.)** From my tensorboard graphs, accuracy and cross entropy loss curves seem to have stablized already. Performance improvement is unlikely or marginal.\n",
    "\n",
    "**4.)** The model does seem overfitted. The cross entropy loss for the training set drops below 0.1 while the cross entropy loss for the dev set stablizes at higher than 0.6. Also, the dev set cross entropy stablizes early in training while training set cross entropy still drops for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization & Tuning\n",
    "\n",
    "The baseline model uses L2 regularization to combat overfitting, but this isn't particularly effective with neural networks since a deep network can still learn spurious logical relationships even with small values for the connection weights. Instead, it's common to use _dropout_, in which we randomly \"drop out\" a subset of the activations by setting them to zero. This prevents units from co-adapting too easily, and often leads to improved generalization\n",
    "\n",
    "**(optional) 5.)** In `models.py`, implement dropout by filling in the missing block in the implementation of `fully_connected_layers(...)`. You'll also need to modify your implementation of `BOW_encoder(...)` to pass the `dropout_rate` and `is_training` parameters to `fully_connected_layers(...)`. \n",
    "\n",
    "**_Do not_** apply dropout to the softmax layer, or to the embeddings.\n",
    "\n",
    "**Hint:** use [`tf.layers.dropout`](https://www.tensorflow.org/api_docs/python/tf/layers/dropout).\n",
    "\n",
    "\n",
    "We've replicated the training code in the cell below - modify `model_params` and `train_params`, and see if you can improve performance with a bit of tuning (_but don't spend too much time on this!_). Some things that might be worth trying:\n",
    "\n",
    "- Enable dropout, and experiment with `dropout_rate`\n",
    "- Train for more epochs (40 or 60). (_But, what happens if you train for too long?_)\n",
    "- Use more hidden layers\n",
    "- Use larger embedding and hidden dimensions\n",
    "- Re-generate the training set with `root_only=False`, which will give set with fine-grained labels\n",
    "\n",
    "\n",
    "**Note:** As it turns out, Naive Bayes is actually a pretty strong model for this dataset and it won't be easy to get a neural model to beat it *(see Table 1 from [Socher et al. 2013](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) - our model is closest in design to the *VecAvg* model)*. Don't worry if tuning doesn't seem to help much for this particular problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fc_with_dropout (models_test.TestFCWithDropout) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.044s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Run this if you implement dropout\n",
    "reload(models)\n",
    "utils.run_tests(models_test, [\"TestFCWithDropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (16,474 words) written to '/tmp/tf_bow_sst_20180211-0753/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20180211-0753/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20180211-0753', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f85e0347240>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20180211-0753' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.4443, step = 1\n",
      "INFO:tensorflow:global_step/sec: 70.1436\n",
      "INFO:tensorflow:loss = 0.82897, step = 101 (1.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.045\n",
      "INFO:tensorflow:loss = 0.713558, step = 201 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8208\n",
      "INFO:tensorflow:loss = 0.583683, step = 301 (1.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2172\n",
      "INFO:tensorflow:loss = 0.470809, step = 401 (1.312 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 433 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.397395.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:53:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-433\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:53:34\n",
      "INFO:tensorflow:Saving dict for global step 433: accuracy = 0.690367, cross_entropy_loss = 0.606229, global_step = 433, loss = 0.791849\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-433\n",
      "INFO:tensorflow:Saving checkpoints for 434 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.244259, step = 434\n",
      "INFO:tensorflow:global_step/sec: 72.12\n",
      "INFO:tensorflow:loss = 0.365883, step = 534 (1.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.2959\n",
      "INFO:tensorflow:loss = 0.366728, step = 634 (1.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.299\n",
      "INFO:tensorflow:loss = 0.365753, step = 734 (1.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.7936\n",
      "INFO:tensorflow:loss = 0.254706, step = 834 (1.318 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 866 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.24976.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:53:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-866\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:53:43\n",
      "INFO:tensorflow:Saving dict for global step 866: accuracy = 0.743119, cross_entropy_loss = 0.582499, global_step = 866, loss = 0.736542\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-866\n",
      "INFO:tensorflow:Saving checkpoints for 867 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.206085, step = 867\n",
      "INFO:tensorflow:global_step/sec: 65.555\n",
      "INFO:tensorflow:loss = 0.2378, step = 967 (1.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.1734\n",
      "INFO:tensorflow:loss = 0.285085, step = 1067 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2168\n",
      "INFO:tensorflow:loss = 0.283716, step = 1167 (1.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.5535\n",
      "INFO:tensorflow:loss = 0.135933, step = 1267 (1.306 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1299 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.118103.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:53:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-1299\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:53:51\n",
      "INFO:tensorflow:Saving dict for global step 1299: accuracy = 0.731651, cross_entropy_loss = 0.73244, global_step = 1299, loss = 0.892462\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-1299\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.129479, step = 1300\n",
      "INFO:tensorflow:global_step/sec: 72.3659\n",
      "INFO:tensorflow:loss = 0.122367, step = 1400 (1.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.3993\n",
      "INFO:tensorflow:loss = 0.216584, step = 1500 (1.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8513\n",
      "INFO:tensorflow:loss = 0.166019, step = 1600 (1.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8824\n",
      "INFO:tensorflow:loss = 0.138522, step = 1700 (1.301 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1732 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0916438.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:53:58\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-1732\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:53:58\n",
      "INFO:tensorflow:Saving dict for global step 1732: accuracy = 0.730505, cross_entropy_loss = 0.929008, global_step = 1732, loss = 1.09204\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-1732\n",
      "INFO:tensorflow:Saving checkpoints for 1733 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.104179, step = 1733\n",
      "INFO:tensorflow:global_step/sec: 78.2524\n",
      "INFO:tensorflow:loss = 0.115627, step = 1833 (1.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.2746\n",
      "INFO:tensorflow:loss = 0.15073, step = 1933 (1.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0447\n",
      "INFO:tensorflow:loss = 0.108654, step = 2033 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0073\n",
      "INFO:tensorflow:loss = 0.118924, step = 2133 (1.316 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2165 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0905561.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:05\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-2165\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:05\n",
      "INFO:tensorflow:Saving dict for global step 2165: accuracy = 0.741973, cross_entropy_loss = 0.815965, global_step = 2165, loss = 0.973288\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-2165\n",
      "INFO:tensorflow:Saving checkpoints for 2166 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.104851, step = 2166\n",
      "INFO:tensorflow:global_step/sec: 70.4499\n",
      "INFO:tensorflow:loss = 0.104313, step = 2266 (1.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5701\n",
      "INFO:tensorflow:loss = 0.12253, step = 2366 (1.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.1745\n",
      "INFO:tensorflow:loss = 0.104163, step = 2466 (1.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9704\n",
      "INFO:tensorflow:loss = 0.118784, step = 2566 (1.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2598 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0849271.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:14\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-2598\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:14\n",
      "INFO:tensorflow:Saving dict for global step 2598: accuracy = 0.739679, cross_entropy_loss = 0.825262, global_step = 2598, loss = 0.975346\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-2598\n",
      "INFO:tensorflow:Saving checkpoints for 2599 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.100721, step = 2599\n",
      "INFO:tensorflow:global_step/sec: 79.6091\n",
      "INFO:tensorflow:loss = 0.100769, step = 2699 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9132\n",
      "INFO:tensorflow:loss = 0.112349, step = 2799 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3831\n",
      "INFO:tensorflow:loss = 0.106779, step = 2899 (1.213 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 81.6812\n",
      "INFO:tensorflow:loss = 0.110953, step = 2999 (1.224 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3031 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0841192.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-3031\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:21\n",
      "INFO:tensorflow:Saving dict for global step 3031: accuracy = 0.738532, cross_entropy_loss = 0.828486, global_step = 3031, loss = 0.97414\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-3031\n",
      "INFO:tensorflow:Saving checkpoints for 3032 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0986938, step = 3032\n",
      "INFO:tensorflow:global_step/sec: 79.2198\n",
      "INFO:tensorflow:loss = 0.099547, step = 3132 (1.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.4601\n",
      "INFO:tensorflow:loss = 0.10801, step = 3232 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7171\n",
      "INFO:tensorflow:loss = 0.104492, step = 3332 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2861\n",
      "INFO:tensorflow:loss = 0.107104, step = 3432 (1.215 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3464 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0835704.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:28\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-3464\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:28\n",
      "INFO:tensorflow:Saving dict for global step 3464: accuracy = 0.743119, cross_entropy_loss = 0.816966, global_step = 3464, loss = 0.959312\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-3464\n",
      "INFO:tensorflow:Saving checkpoints for 3465 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0965555, step = 3465\n",
      "INFO:tensorflow:global_step/sec: 74.1722\n",
      "INFO:tensorflow:loss = 0.0984642, step = 3565 (1.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8134\n",
      "INFO:tensorflow:loss = 0.105809, step = 3665 (1.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.2191\n",
      "INFO:tensorflow:loss = 0.103772, step = 3765 (1.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.3525\n",
      "INFO:tensorflow:loss = 0.105694, step = 3865 (1.327 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3897 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0829348.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:36\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-3897\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:36\n",
      "INFO:tensorflow:Saving dict for global step 3897: accuracy = 0.74656, cross_entropy_loss = 0.809054, global_step = 3897, loss = 0.948837\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-3897\n",
      "INFO:tensorflow:Saving checkpoints for 3898 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0948516, step = 3898\n",
      "INFO:tensorflow:global_step/sec: 72.8126\n",
      "INFO:tensorflow:loss = 0.0972656, step = 3998 (1.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.666\n",
      "INFO:tensorflow:loss = 0.103915, step = 4098 (1.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9358\n",
      "INFO:tensorflow:loss = 0.102787, step = 4198 (1.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.956\n",
      "INFO:tensorflow:loss = 0.104507, step = 4298 (1.300 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4330 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0824556.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-4330\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:44\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.747706, cross_entropy_loss = 0.802351, global_step = 4330, loss = 0.940155\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-4330\n",
      "INFO:tensorflow:Saving checkpoints for 4331 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0936199, step = 4331\n",
      "INFO:tensorflow:global_step/sec: 72.6392\n",
      "INFO:tensorflow:loss = 0.0964938, step = 4431 (1.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.3024\n",
      "INFO:tensorflow:loss = 0.102427, step = 4531 (1.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.2254\n",
      "INFO:tensorflow:loss = 0.102058, step = 4631 (1.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0739\n",
      "INFO:tensorflow:loss = 0.103673, step = 4731 (1.298 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4763 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0819951.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-4763\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:51\n",
      "INFO:tensorflow:Saving dict for global step 4763: accuracy = 0.74656, cross_entropy_loss = 0.796125, global_step = 4763, loss = 0.932329\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-4763\n",
      "INFO:tensorflow:Saving checkpoints for 4764 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0926473, step = 4764\n",
      "INFO:tensorflow:global_step/sec: 72.9681\n",
      "INFO:tensorflow:loss = 0.0959128, step = 4864 (1.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.9586\n",
      "INFO:tensorflow:loss = 0.101266, step = 4964 (1.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.3203\n",
      "INFO:tensorflow:loss = 0.101402, step = 5064 (1.553 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.1041\n",
      "INFO:tensorflow:loss = 0.102993, step = 5164 (1.387 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5196 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0815891.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:54:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-5196\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:54:59\n",
      "INFO:tensorflow:Saving dict for global step 5196: accuracy = 0.74656, cross_entropy_loss = 0.790733, global_step = 5196, loss = 0.925635\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-5196\n",
      "INFO:tensorflow:Saving checkpoints for 5197 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0918556, step = 5197\n",
      "INFO:tensorflow:global_step/sec: 68.8116\n",
      "INFO:tensorflow:loss = 0.0954855, step = 5297 (1.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.4908\n",
      "INFO:tensorflow:loss = 0.100338, step = 5397 (1.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.2855\n",
      "INFO:tensorflow:loss = 0.100844, step = 5497 (1.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.6002\n",
      "INFO:tensorflow:loss = 0.10243, step = 5597 (1.340 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5629 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.081218.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-5629\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:08\n",
      "INFO:tensorflow:Saving dict for global step 5629: accuracy = 0.745413, cross_entropy_loss = 0.785845, global_step = 5629, loss = 0.919663\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-5629\n",
      "INFO:tensorflow:Saving checkpoints for 5630 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0911966, step = 5630\n",
      "INFO:tensorflow:global_step/sec: 70.9373\n",
      "INFO:tensorflow:loss = 0.0951445, step = 5730 (1.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.7995\n",
      "INFO:tensorflow:loss = 0.0995754, step = 5830 (1.319 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.7725\n",
      "INFO:tensorflow:loss = 0.100366, step = 5930 (1.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0085\n",
      "INFO:tensorflow:loss = 0.101948, step = 6030 (1.316 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6062 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.0808796.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-6062\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:15\n",
      "INFO:tensorflow:Saving dict for global step 6062: accuracy = 0.744266, cross_entropy_loss = 0.781448, global_step = 6062, loss = 0.914353\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-6062\n",
      "INFO:tensorflow:Saving checkpoints for 6063 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0906378, step = 6063\n",
      "INFO:tensorflow:global_step/sec: 73.1613\n",
      "INFO:tensorflow:loss = 0.0948631, step = 6163 (1.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.584\n",
      "INFO:tensorflow:loss = 0.0989347, step = 6263 (1.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0977\n",
      "INFO:tensorflow:loss = 0.099958, step = 6363 (1.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.1594\n",
      "INFO:tensorflow:loss = 0.101531, step = 6463 (1.296 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6495 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0805677.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-6495\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:23\n",
      "INFO:tensorflow:Saving dict for global step 6495: accuracy = 0.741973, cross_entropy_loss = 0.77744, global_step = 6495, loss = 0.909566\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-6495\n",
      "INFO:tensorflow:Saving checkpoints for 6496 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0901549, step = 6496\n",
      "INFO:tensorflow:global_step/sec: 73.377\n",
      "INFO:tensorflow:loss = 0.0946195, step = 6596 (1.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.1093\n",
      "INFO:tensorflow:loss = 0.0983853, step = 6696 (1.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.1158\n",
      "INFO:tensorflow:loss = 0.0996068, step = 6796 (1.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.1509\n",
      "INFO:tensorflow:loss = 0.101167, step = 6896 (1.313 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6928 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0802787.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-6928\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:31\n",
      "INFO:tensorflow:Saving dict for global step 6928: accuracy = 0.744266, cross_entropy_loss = 0.773779, global_step = 6928, loss = 0.905234\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-6928\n",
      "INFO:tensorflow:Saving checkpoints for 6929 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0897304, step = 6929\n",
      "INFO:tensorflow:global_step/sec: 69.2732\n",
      "INFO:tensorflow:loss = 0.0944026, step = 7029 (1.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.4249\n",
      "INFO:tensorflow:loss = 0.0979064, step = 7129 (1.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.3429\n",
      "INFO:tensorflow:loss = 0.0993025, step = 7229 (1.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.8583\n",
      "INFO:tensorflow:loss = 0.100847, step = 7329 (1.319 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7361 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0800098.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-7361\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:39\n",
      "INFO:tensorflow:Saving dict for global step 7361: accuracy = 0.744266, cross_entropy_loss = 0.770414, global_step = 7361, loss = 0.901288\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-7361\n",
      "INFO:tensorflow:Saving checkpoints for 7362 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0893515, step = 7362\n",
      "INFO:tensorflow:global_step/sec: 72.9552\n",
      "INFO:tensorflow:loss = 0.0942053, step = 7462 (1.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.2735\n",
      "INFO:tensorflow:loss = 0.0974834, step = 7562 (1.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.9291\n",
      "INFO:tensorflow:loss = 0.099037, step = 7662 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.812\n",
      "INFO:tensorflow:loss = 0.100567, step = 7762 (1.302 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7794 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0797586.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-7794\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:46\n",
      "INFO:tensorflow:Saving dict for global step 7794: accuracy = 0.747706, cross_entropy_loss = 0.76731, global_step = 7794, loss = 0.897674\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-7794\n",
      "INFO:tensorflow:Saving checkpoints for 7795 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0890091, step = 7795\n",
      "INFO:tensorflow:global_step/sec: 73.0208\n",
      "INFO:tensorflow:loss = 0.094024, step = 7895 (1.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4236\n",
      "INFO:tensorflow:loss = 0.097106, step = 7995 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.8678\n",
      "INFO:tensorflow:loss = 0.0988035, step = 8095 (1.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.5979\n",
      "INFO:tensorflow:loss = 0.100319, step = 8195 (1.306 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8227 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0795234.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:55:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-8227\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:55:54\n",
      "INFO:tensorflow:Saving dict for global step 8227: accuracy = 0.74656, cross_entropy_loss = 0.764432, global_step = 8227, loss = 0.894347\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-8227\n",
      "INFO:tensorflow:Saving checkpoints for 8228 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0886966, step = 8228\n",
      "INFO:tensorflow:global_step/sec: 80.1614\n",
      "INFO:tensorflow:loss = 0.0938564, step = 8328 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0873\n",
      "INFO:tensorflow:loss = 0.0967665, step = 8428 (1.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6678\n",
      "INFO:tensorflow:loss = 0.098597, step = 8528 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.588\n",
      "INFO:tensorflow:loss = 0.100101, step = 8628 (1.182 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8660 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.079303.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:01\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-8660\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:01\n",
      "INFO:tensorflow:Saving dict for global step 8660: accuracy = 0.745413, cross_entropy_loss = 0.761751, global_step = 8660, loss = 0.891268\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-8660\n",
      "INFO:tensorflow:Saving checkpoints for 8661 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0884096, step = 8661\n",
      "INFO:tensorflow:global_step/sec: 80.4303\n",
      "INFO:tensorflow:loss = 0.0937012, step = 8761 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8536\n",
      "INFO:tensorflow:loss = 0.096459, step = 8861 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1743\n",
      "INFO:tensorflow:loss = 0.0984132, step = 8961 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5977\n",
      "INFO:tensorflow:loss = 0.0999069, step = 9061 (1.196 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9093 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.079096.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-9093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:08\n",
      "INFO:tensorflow:Saving dict for global step 9093: accuracy = 0.745413, cross_entropy_loss = 0.759242, global_step = 9093, loss = 0.888405\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-9093\n",
      "INFO:tensorflow:Saving checkpoints for 9094 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0881446, step = 9094\n",
      "INFO:tensorflow:global_step/sec: 80.5323\n",
      "INFO:tensorflow:loss = 0.0935572, step = 9194 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6153\n",
      "INFO:tensorflow:loss = 0.0961791, step = 9294 (1.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1875\n",
      "INFO:tensorflow:loss = 0.0982487, step = 9394 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8961\n",
      "INFO:tensorflow:loss = 0.0997345, step = 9494 (1.206 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9526 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0789016.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:14\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-9526\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:15\n",
      "INFO:tensorflow:Saving dict for global step 9526: accuracy = 0.747706, cross_entropy_loss = 0.756884, global_step = 9526, loss = 0.88573\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-9526\n",
      "INFO:tensorflow:Saving checkpoints for 9527 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0878992, step = 9527\n",
      "INFO:tensorflow:global_step/sec: 81.3531\n",
      "INFO:tensorflow:loss = 0.0934234, step = 9627 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7727\n",
      "INFO:tensorflow:loss = 0.0959231, step = 9727 (1.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.7783\n",
      "INFO:tensorflow:loss = 0.098101, step = 9827 (1.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0843\n",
      "INFO:tensorflow:loss = 0.0995803, step = 9927 (1.298 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9959 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0787186.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-9959\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:23\n",
      "INFO:tensorflow:Saving dict for global step 9959: accuracy = 0.748853, cross_entropy_loss = 0.75466, global_step = 9959, loss = 0.883222\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-9959\n",
      "INFO:tensorflow:Saving checkpoints for 9960 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0876716, step = 9960\n",
      "INFO:tensorflow:global_step/sec: 71.9206\n",
      "INFO:tensorflow:loss = 0.0932987, step = 10060 (1.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.5828\n",
      "INFO:tensorflow:loss = 0.095688, step = 10160 (1.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4449\n",
      "INFO:tensorflow:loss = 0.0979677, step = 10260 (1.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.041\n",
      "INFO:tensorflow:loss = 0.099442, step = 10360 (1.219 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10392 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0785462.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-10392\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:30\n",
      "INFO:tensorflow:Saving dict for global step 10392: accuracy = 0.747706, cross_entropy_loss = 0.752553, global_step = 10392, loss = 0.880859\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-10392\n",
      "INFO:tensorflow:Saving checkpoints for 10393 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0874603, step = 10393\n",
      "INFO:tensorflow:global_step/sec: 79.5377\n",
      "INFO:tensorflow:loss = 0.0931821, step = 10493 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1622\n",
      "INFO:tensorflow:loss = 0.0954713, step = 10593 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.61\n",
      "INFO:tensorflow:loss = 0.0978471, step = 10693 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9877\n",
      "INFO:tensorflow:loss = 0.0993175, step = 10793 (1.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10825 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0783836.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:36\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-10825\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:37\n",
      "INFO:tensorflow:Saving dict for global step 10825: accuracy = 0.747706, cross_entropy_loss = 0.750551, global_step = 10825, loss = 0.878627\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-10825\n",
      "INFO:tensorflow:Saving checkpoints for 10826 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.087264, step = 10826\n",
      "INFO:tensorflow:global_step/sec: 72.232\n",
      "INFO:tensorflow:loss = 0.0930725, step = 10926 (1.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.531\n",
      "INFO:tensorflow:loss = 0.0952708, step = 11026 (1.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.7834\n",
      "INFO:tensorflow:loss = 0.0977377, step = 11126 (1.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7037\n",
      "INFO:tensorflow:loss = 0.0992048, step = 11226 (1.414 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11258 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0782301.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:44\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-11258\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:44\n",
      "INFO:tensorflow:Saving dict for global step 11258: accuracy = 0.748853, cross_entropy_loss = 0.748643, global_step = 11258, loss = 0.87651\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-11258\n",
      "INFO:tensorflow:Saving checkpoints for 11259 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0870819, step = 11259\n",
      "INFO:tensorflow:global_step/sec: 71.5265\n",
      "INFO:tensorflow:loss = 0.092969, step = 11359 (1.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.725\n",
      "INFO:tensorflow:loss = 0.0950847, step = 11459 (1.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.6358\n",
      "INFO:tensorflow:loss = 0.097638, step = 11559 (1.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.6052\n",
      "INFO:tensorflow:loss = 0.0991025, step = 11659 (1.323 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11691 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0780848.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:56:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-11691\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:56:53\n",
      "INFO:tensorflow:Saving dict for global step 11691: accuracy = 0.74656, cross_entropy_loss = 0.746819, global_step = 11691, loss = 0.874496\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-11691\n",
      "INFO:tensorflow:Saving checkpoints for 11692 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0869129, step = 11692\n",
      "INFO:tensorflow:global_step/sec: 72.6474\n",
      "INFO:tensorflow:loss = 0.0928708, step = 11792 (1.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.7109\n",
      "INFO:tensorflow:loss = 0.0949114, step = 11892 (1.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.2288\n",
      "INFO:tensorflow:loss = 0.0975471, step = 11992 (1.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9597\n",
      "INFO:tensorflow:loss = 0.0990093, step = 12092 (1.299 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12124 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0779473.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:57:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-12124\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:57:01\n",
      "INFO:tensorflow:Saving dict for global step 12124: accuracy = 0.74656, cross_entropy_loss = 0.745072, global_step = 12124, loss = 0.872577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-12124\n",
      "INFO:tensorflow:Saving checkpoints for 12125 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0867562, step = 12125\n",
      "INFO:tensorflow:global_step/sec: 72.4472\n",
      "INFO:tensorflow:loss = 0.0927769, step = 12225 (1.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.1289\n",
      "INFO:tensorflow:loss = 0.0947497, step = 12325 (1.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.1643\n",
      "INFO:tensorflow:loss = 0.0974639, step = 12425 (1.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.4365\n",
      "INFO:tensorflow:loss = 0.098924, step = 12525 (1.308 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12557 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0778168.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:57:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-12557\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:57:08\n",
      "INFO:tensorflow:Saving dict for global step 12557: accuracy = 0.745413, cross_entropy_loss = 0.743396, global_step = 12557, loss = 0.870743\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-12557\n",
      "INFO:tensorflow:Saving checkpoints for 12558 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0866109, step = 12558\n",
      "INFO:tensorflow:global_step/sec: 72.2679\n",
      "INFO:tensorflow:loss = 0.0926868, step = 12658 (1.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.1513\n",
      "INFO:tensorflow:loss = 0.0945982, step = 12758 (1.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5542\n",
      "INFO:tensorflow:loss = 0.0973876, step = 12858 (1.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.4576\n",
      "INFO:tensorflow:loss = 0.0988457, step = 12958 (1.325 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12990 into /tmp/tf_bow_sst_20180211-0753/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0776928.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:57:15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-12990\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:57:16\n",
      "INFO:tensorflow:Saving dict for global step 12990: accuracy = 0.745413, cross_entropy_loss = 0.741783, global_step = 12990, loss = 0.868986\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=ds.vocab.size, embed_dim=70, hidden_dims=[35,35], num_classes=len(ds.target_names),\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01, dropout_rate=0.25)  # fill this in\n",
    "\n",
    "# Specify training schedule\n",
    "train_params = dict(batch_size=32, total_epochs=60, eval_every=2)  # fill this in\n",
    "\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "###\n",
    "# Don't change anything below this line\n",
    "###\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir): shutil.rmtree(checkpoint_dir)\n",
    "ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, params=model_params, model_dir=checkpoint_dir)\n",
    "print(\"\\nTo view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\\n\")\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42)\n",
    "dev_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False)\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-11-07:58:11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180211-0753/model.ckpt-12990\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-11-07:58:12\n",
      "INFO:tensorflow:Saving dict for global step 12990: accuracy = 0.769358, cross_entropy_loss = 0.704299, global_step = 12990, loss = 0.830208\n",
      "Accuracy on test set: 76.94%\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False)\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
