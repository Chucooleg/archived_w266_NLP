{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Word Embeddings\n",
    "\n",
    "In this part of the assignment, we'll explore a few properties of word embeddings. We'll use pre-trained GloVe ([Pennington et al. 2013](https://nlp.stanford.edu/pubs/glove.pdf)) embeddings, and evaluate on the analogy task described in ([Mikolov et al. 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)).\n",
    "\n",
    "If you haven't seen the [embeddings.ipynb](../../../materials/embeddings/embeddings.ipynb) demo notebook, we recommend you look through it; this part of the assignment will build on that material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install a few python packages using pip\n",
    "from w266_common import utils\n",
    "utils.require_package(\"wget\")      # for fetching dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeunghoman/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits like a GloVe\n",
    "\n",
    "Word embeddings take a long time to train - since the goal is to provide a good representation for as many words as possible, generating good embeddings often requires making several passes over a very large corpus. \n",
    "\n",
    "Fortunately, it's possible to learn fairly general embeddings from large corpora that are useful for many downstream tasks. We'll use the GloVe vectors available at https://nlp.stanford.edu/projects/glove/ - specifically, a set trained with a vocabulary of 400,000 on a corpus of 6B tokens from Wikipedia and Gigaword.\n",
    "\n",
    "The vectors are distributed as a (very) large text file, with one word per line followed by its vector:\n",
    "```\n",
    "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459\n",
    "```\n",
    "\n",
    "We've implemented a helper class, `Hands` in `glove_helper.py`, that will parse these files in a memory efficient manner and provide a wrapper object over a NumPy array containing the actual vectors. \n",
    "\n",
    "Run the cell below; the first time, it will download an ~800 MB file to the `data/` directory. **_Please do not check this in to git!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "import glove_helper; reload(glove_helper)\n",
    "\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hands` has a few properties and methods that might be useful:\n",
    "- `hands.vocab` is a `vocabulary.Vocabulary` object that manages the set of available words\n",
    "- `hands.W` is a matrix of shape $|V| \\times d$ containing the actual vectors, one per row. Row indices are as given by `hands.vocab.word_to_id[word]`.\n",
    "- `hands.get_vector(word)` returns the vector for a word (passed as a string).\n",
    "\n",
    "Note that we let $|V| = $`hands.W.shape[0]`, which in addition to the actual words includes three special tokens: `<s>` (begin sentence), `</s>` (end sentence), and `<unk>` (unknown word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<w266_common.vocabulary.Vocabulary at 0x7f0bc94740f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05209883, -0.09711445, -0.1380765 , ...,  0.12381283,\n",
       "        -0.23434106, -0.00925518],\n",
       "       [ 0.05209883, -0.09711445, -0.1380765 , ...,  0.12381283,\n",
       "        -0.23434106, -0.00925518],\n",
       "       [ 0.05209883, -0.09711445, -0.1380765 , ...,  0.12381283,\n",
       "        -0.23434106, -0.00925518],\n",
       "       ..., \n",
       "       [ 0.36087999, -0.16919   , -0.32703999, ...,  0.27138999,\n",
       "        -0.29188001,  0.16109   ],\n",
       "       [-0.10461   , -0.50470001, -0.49331   , ...,  0.42526999,\n",
       "        -0.51249999, -0.17054   ],\n",
       "       [ 0.28365001, -0.62629998, -0.44351   , ...,  0.43678001,\n",
       "        -0.82607001, -0.15701   ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400003, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.vocab.word_to_id[\"geek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.08549997e-01,   2.05390006e-01,   9.30869997e-01,\n",
       "        -1.21159995e+00,  -3.63279998e-01,   5.88349998e-01,\n",
       "         1.24959998e-01,  -9.69470013e-03,   3.54460001e-01,\n",
       "         9.26100016e-01,  -3.95599991e-01,  -3.21720004e-01,\n",
       "        -3.35909992e-01,  -6.74260035e-02,   7.68050030e-02,\n",
       "         7.65829980e-01,   6.58720016e-01,   2.09779993e-01,\n",
       "        -2.53639996e-01,   5.18010020e-01,  -2.09670007e-01,\n",
       "        -2.16020003e-01,  -6.55939996e-01,  -5.50639987e-01,\n",
       "         2.71349996e-01,   1.54489994e-01,   2.28990003e-01,\n",
       "         1.84009999e-01,  -4.96740006e-02,   1.47630006e-01,\n",
       "        -1.33540004e-01,  -1.23460002e-01,  -2.11300001e-01,\n",
       "        -1.41900003e-01,   4.23830003e-01,   3.04459989e-01,\n",
       "        -8.78130019e-01,   1.59799993e-01,   3.66650000e-02,\n",
       "        -6.43159986e-01,  -4.54999991e-02,   1.01970002e-01,\n",
       "        -3.19700003e-01,  -5.69100022e-01,  -1.00000001e-01,\n",
       "        -2.74030000e-01,  -3.81949991e-01,   8.14469993e-01,\n",
       "         5.61450005e-01,  -2.77629998e-02,  -2.00650007e-01,\n",
       "        -4.13080007e-02,   9.29049999e-02,  -4.97170001e-01,\n",
       "        -6.66649997e-01,  -4.42680001e-01,   1.61300004e-01,\n",
       "         1.67480007e-01,   3.67170006e-01,   5.42449988e-02,\n",
       "         8.68860006e-01,   8.85609984e-02,  -8.73579979e-01,\n",
       "        -1.94940001e-01,  -3.23229998e-01,   2.96970010e-01,\n",
       "         9.73590016e-01,  -1.49869993e-01,   2.04840004e-01,\n",
       "         2.76259989e-01,   5.39629996e-01,  -5.18100001e-02,\n",
       "        -2.24120006e-01,   1.19139999e-01,  -1.32770002e+00,\n",
       "         2.20660001e-01,   2.10189998e-01,  -4.67099994e-01,\n",
       "         4.34309989e-01,  -3.70200008e-01,  -4.92590010e-01,\n",
       "        -4.27309982e-02,   1.11359999e-01,  -6.81660022e-04,\n",
       "        -5.89410007e-01,   1.18960001e-01,  -4.71190006e-01,\n",
       "        -3.13400000e-01,  -4.16070014e-01,  -1.97080001e-01,\n",
       "         2.28190005e-01,   1.20299995e+00,   4.14079994e-01,\n",
       "        -1.53689995e-01,   2.36320004e-01,  -2.78629988e-01,\n",
       "        -1.51190003e-02,  -6.53900027e-01,  -3.13879992e-03,\n",
       "         4.13670003e-01], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.get_vector(\"geek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a): Nearest Neighbors\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To measure the similarity of two words, we'll use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between their representation vectors:\n",
    "\n",
    "$$ D^{cos}_{ij} = \\frac{v_i^T v_j}{||v_i||\\ ||v_j||}$$\n",
    "\n",
    "*Note that this is called cosine similarity because $D^{cos}_{ij} = \\cos(\\theta_{ij})$, where $\\theta_{ij}$ is the angle between the two vectors.*\n",
    "\n",
    "## Part (a) Questions\n",
    "\n",
    "1. In `vector_math.py`, implement the `find_nn_cos(...)` function. Read the docstring _carefully_ - it describes what you should return. *Hint: use NumPy functions instead of a `for` loop.*\n",
    "<p>\n",
    "2. Use the `show_nns(...)` function below to find the nearest neighbors for the words `\"bank\"`, `\"plane\"`, and `\"flies\"`. Are the neighbors dominated by one sense of these words or another? Is there evidence that the vector encodes meaning of the other senses as well?\n",
    "<p>\n",
    "3. Like `word2vec`, GloVe constructs representations by summarizing word-word coocurrence statistics. Use `show_nns(...)` to find the neighbors of `\"green\"` and `\"celadon\"`, and `\"orange\"` and `\"ochre\"`. Explain what you find in terms of the distributional hypothesis and the grounding problem. Do the vectors for `\"ochre\"` and `\"celadon\"` appear to encode a notion of color? What do they represent, instead?\n",
    "\n",
    "_(Recall that the Distributional Hypothesis is the idea that \"you shall know a word by the company it keeps\" (Firth, 1957) - that meaning is derived from the context in which a word is used. Grounding refers to the meaning of language in terms of external concepts, such as real-world entities or physical characteristics.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'the'\n",
      "1.000 : 'the'\n",
      "0.857 : 'this'\n",
      "0.851 : 'part'\n",
      "0.850 : 'one'\n",
      "0.833 : 'of'\n",
      "0.832 : 'same'\n",
      "0.821 : 'first'\n",
      "0.820 : 'on'\n",
      "0.817 : 'its'\n",
      "0.813 : 'as'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vector_math; reload(vector_math)\n",
    "\n",
    "def show_nns(hands, word, k=10):\n",
    "    \"\"\"Helper function to print neighbors of a given word.\"\"\"\n",
    "    word = word.lower()\n",
    "    print(\"Nearest neighbors for '{:s}'\".format(word))\n",
    "    v = hands.get_vector(word)\n",
    "    for i, sim in zip(*vector_math.find_nn_cos(v, hands.W, k)):\n",
    "        target_word = hands.vocab.id_to_word[i]\n",
    "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
    "    print(\"\")\n",
    "    \n",
    "show_nns(hands, \"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'bank'\n",
      "1.000 : 'bank'\n",
      "0.806 : 'banks'\n",
      "0.753 : 'banking'\n",
      "0.704 : 'credit'\n",
      "0.694 : 'investment'\n",
      "0.678 : 'financial'\n",
      "0.669 : 'securities'\n",
      "0.665 : 'lending'\n",
      "0.648 : 'funds'\n",
      "0.648 : 'ubs'\n",
      "\n",
      "None\n",
      "Nearest neighbors for 'place'\n",
      "1.000 : 'place'\n",
      "0.807 : 'time'\n",
      "0.795 : 'only'\n",
      "0.785 : 'one'\n",
      "0.784 : 'take'\n",
      "0.780 : 'next'\n",
      "0.780 : 'this'\n",
      "0.772 : 'the'\n",
      "0.770 : 'places'\n",
      "0.765 : 'where'\n",
      "\n",
      "None\n",
      "Nearest neighbors for 'flies'\n",
      "1.000 : 'flies'\n",
      "0.741 : 'fly'\n",
      "0.644 : 'flying'\n",
      "0.634 : 'insects'\n",
      "0.632 : 'flew'\n",
      "0.618 : 'butterflies'\n",
      "0.614 : 'moths'\n",
      "0.609 : 'moth'\n",
      "0.581 : 'planes'\n",
      "0.576 : 'plane'\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).2\n",
    "print (show_nns(hands, \"bank\"))\n",
    "print (show_nns(hands, \"place\"))\n",
    "print (show_nns(hands, \"flies\"))\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Under `\"bank\"`, 7 of the 9 neighbors pertain to the meaning of financial services. Only `\"banks\"` and `\"ubs\"` pertain closer to the meaning of a bank as a place. Under `\"place\"`, the neighbors meanings are more spread, with only `\"places\" and \"where\"` to relate to locations, many others are determiners. Under `\"flies\"`, the nieghbors either refer to insects, the action of flying, or flights related. The meanings are spread but make relatable. Therefore, the vectors certainly encode multiple word senses, not restricted to be one way or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'green'\n",
      "1.000 : 'green'\n",
      "0.820 : 'red'\n",
      "0.787 : 'blue'\n",
      "0.781 : 'brown'\n",
      "0.771 : 'yellow'\n",
      "0.762 : 'white'\n",
      "0.749 : 'gray'\n",
      "0.733 : 'black'\n",
      "0.729 : 'pink'\n",
      "0.728 : 'purple'\n",
      "\n",
      "None\n",
      "Nearest neighbors for 'celadon'\n",
      "1.000 : 'celadon'\n",
      "0.620 : 'faience'\n",
      "0.602 : 'porcelains'\n",
      "0.594 : 'majolica'\n",
      "0.591 : 'ocher'\n",
      "0.585 : 'blue-and-white'\n",
      "0.575 : 'glazes'\n",
      "0.563 : 'unglazed'\n",
      "0.558 : 'porcelain'\n",
      "0.549 : 'steatite'\n",
      "\n",
      "None\n",
      "Nearest neighbors for 'orange'\n",
      "1.000 : 'orange'\n",
      "0.736 : 'yellow'\n",
      "0.714 : 'red'\n",
      "0.712 : 'blue'\n",
      "0.711 : 'green'\n",
      "0.678 : 'pink'\n",
      "0.677 : 'purple'\n",
      "0.671 : 'black'\n",
      "0.665 : 'colored'\n",
      "0.625 : 'lemon'\n",
      "\n",
      "None\n",
      "Nearest neighbors for 'ochre'\n",
      "1.000 : 'ochre'\n",
      "0.687 : 'pigment'\n",
      "0.677 : 'reddish'\n",
      "0.674 : 'ocher'\n",
      "0.662 : 'coloured'\n",
      "0.658 : 'greenish'\n",
      "0.648 : 'magenta'\n",
      "0.634 : 'pigments'\n",
      "0.632 : 'yellowish'\n",
      "0.629 : 'mottled'\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).3\n",
    "print (show_nns(hands, \"green\"))\n",
    "print (show_nns(hands, \"celadon\"))\n",
    "print (show_nns(hands, \"orange\"))\n",
    "print (show_nns(hands, \"ochre\"))\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Some color words are often used in very specific contexts. For example, `\"celadon\"` is often used to refer to porcelain, so its neighbors are mostly words related to porcelains rather than colors. `\"ochre\"` is often used in the color dying context so its neighbors are mostly words related to dying rather than colors. As such, the problem with distributional representations is encoding by context can loose precision about the words' intrinsic meaning. Words representations can be grounded by real-world external concepts and unable to adpapt to new and possible concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (b): Linear Analogies\n",
    "\n",
    "In this part, you'll implement the word analogy task described in Section 4 of ([Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781.pdf)), and discussed in section 4.8 and 4.11 of the async.\n",
    "\n",
    "1. In `vector_math.py`, implement the `analogy(...)` function. (*Hint: this should be a very short function, given what you've already written above.*)\n",
    "<p>\n",
    "2. Evaluate a few analogies using the `show_analogy(...)` function below. In particular, find at least one analogy that tests each of the following relationships, and that the model gets right:<ul>\n",
    "<li> Singular / plural\n",
    "<li> Superlatives\n",
    "<li> Verb tense\n",
    "<li> Country / capital\n",
    "</ul>\n",
    "(See Table 1 of ([Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781.pdf)) for a few ideas)\n",
    "<p>\n",
    "3. Evaluate the following analogies:\n",
    "<ul>\n",
    "<li> `\"lizard\" is to \"reptile\" as \"dog\" is to ____`\n",
    "<li> `\"finger\" is to  \"hand\"   as \"toe\" is to ____`\n",
    "</ul>\n",
    "What types of relations do these test? (*Hint: think back to WordNet, and things that end in -nymy.*) Does our approach of linear analogies work well here? What assumption is violated by these sorts of relationships? (*Hint: what if we reversed the order, and tested \"reptile\" is to \"lizard\", and so on?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vector_math; reload(vector_math)\n",
    "\n",
    "def show_analogy(hands, a, b, c, k=5):\n",
    "    \"\"\"Compute and print a vector analogy.\"\"\"\n",
    "    a, b, c = a.lower(), b.lower(), c.lower()\n",
    "    va = hands.get_vector(a)\n",
    "    vb = hands.get_vector(b)\n",
    "    vc = hands.get_vector(c)\n",
    "    print(\"'{a:s}' is to '{b:s}' as '{c:s}' is to ___\".format(**locals()))\n",
    "    for i, sim in zip(*vector_math.analogy(va, vb, vc, hands.W, k)):\n",
    "        target_word = hands.vocab.id_to_word[i]\n",
    "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'king' is to 'queen' as 'man' is to ___\n",
      "0.804 : 'woman'\n",
      "0.779 : 'man'\n",
      "0.735 : 'girl'\n",
      "0.682 : 'she'\n",
      "0.659 : 'her'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_analogy(hands, \"king\", \"queen\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Singular/Plural:\n",
      "'mouse' is to 'mice' as 'horse' is to ___\n",
      "0.823 : 'horses'\n",
      "0.750 : 'horse'\n",
      "0.614 : 'breeders'\n",
      "0.611 : 'cows'\n",
      "0.609 : 'thoroughbred'\n",
      "\n",
      "None\n",
      "----------------------------------------\n",
      "Superlatives:\n",
      "'thin' is to 'thinnest' as 'dense' is to ___\n",
      "0.681 : 'densest'\n",
      "0.644 : 'thinnest'\n",
      "0.631 : 'rainforests'\n",
      "0.571 : 'undergrowth'\n",
      "0.562 : 'sub-tropical'\n",
      "\n",
      "None\n",
      "----------------------------------------\n",
      "Verb tense:\n",
      "'eat' is to 'ate' as 'go' is to ___\n",
      "0.845 : 'went'\n",
      "0.798 : 'came'\n",
      "0.791 : 'gone'\n",
      "0.772 : 'got'\n",
      "0.748 : 'going'\n",
      "\n",
      "None\n",
      "----------------------------------------\n",
      "Country / capital:\n",
      "'china' is to 'beijing' as 'india' is to ___\n",
      "0.897 : 'delhi'\n",
      "0.817 : 'india'\n",
      "0.714 : 'islamabad'\n",
      "0.707 : 'pakistan'\n",
      "0.665 : 'lahore'\n",
      "\n",
      "None\n",
      "'japan' is to 'tokyo' as 'australia' is to ___\n",
      "0.833 : 'sydney'\n",
      "0.768 : 'london'\n",
      "0.755 : 'melbourne'\n",
      "0.723 : 'australia'\n",
      "0.714 : 'perth'\n",
      "\n",
      "None\n",
      "'peru' is to 'lima' as 'korea' is to ___\n",
      "0.791 : 'seoul'\n",
      "0.774 : 'korea'\n",
      "0.771 : 'pyongyang'\n",
      "0.730 : 'korean'\n",
      "0.672 : 'kim'\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (b).2\n",
    "print ()\n",
    "print (\"----------------------------------------\")\n",
    "print (\"Singular/Plural:\")\n",
    "print (show_analogy(hands, \"mouse\", \"mice\", \"horse\"))\n",
    "print (\"----------------------------------------\")\n",
    "print (\"Superlatives:\")\n",
    "print (show_analogy(hands, \"thin\", \"thinnest\", \"dense\"))\n",
    "print (\"----------------------------------------\")\n",
    "print (\"Verb tense:\")\n",
    "print (show_analogy(hands, \"eat\", \"ate\", \"go\"))\n",
    "print (\"----------------------------------------\")\n",
    "print (\"Country / capital:\")\n",
    "print (show_analogy(hands, \"China\", \"Beijing\", \"India\"))\n",
    "print (show_analogy(hands, \"Japan\", \"Tokyo\", \"Australia\"))\n",
    "print (show_analogy(hands, \"Peru\", \"Lima\", \"Korea\"))\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. The model doesn't always get the relationships right, afterall we modeled the distributed representation for each word based on context rather than the more strict semantic relationships. Contextual relationships are not exactly logical relationships. Contextural relationships are less static and more diverse -- multiple relationships can exist between two words and the same relationship can exist between one word and many other candidates, while logical relationships are rigid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lizard' is to 'reptile' as 'dog' is to ___\n",
      "0.768 : 'dog'\n",
      "0.682 : 'dogs'\n",
      "0.662 : 'pet'\n",
      "0.635 : 'puppy'\n",
      "0.629 : 'animal'\n",
      "0.617 : 'cat'\n",
      "0.575 : 'pets'\n",
      "0.574 : 'reptile'\n",
      "0.556 : 'petting'\n",
      "0.542 : 'animals'\n",
      "0.542 : 'herd'\n",
      "0.538 : 'hog'\n",
      "0.531 : 'hunting'\n",
      "0.526 : 'toy'\n",
      "0.526 : 'horse'\n",
      "0.525 : 'cow'\n",
      "0.521 : 'canine'\n",
      "0.521 : 'bird'\n",
      "0.514 : 'dinosaur'\n",
      "0.505 : 'moose'\n",
      "0.503 : 'breed'\n",
      "0.499 : 'pig'\n",
      "0.497 : 'sled'\n",
      "0.494 : 'cats'\n",
      "0.492 : 'handlers'\n",
      "0.490 : 'backyard'\n",
      "0.490 : 'mad'\n",
      "0.490 : 'meat'\n",
      "0.490 : 'horses'\n",
      "0.487 : 'hobby'\n",
      "\n",
      "None\n",
      "'finger' is to 'hand' as 'toe' is to ___\n",
      "0.776 : 'toe'\n",
      "0.625 : 'hand'\n",
      "0.581 : 'shoes'\n",
      "0.567 : 'back'\n",
      "0.566 : 'hands'\n",
      "0.562 : 'wear'\n",
      "0.561 : 'face'\n",
      "0.555 : 'shoulder'\n",
      "0.549 : 'heel'\n",
      "0.547 : 'wearing'\n",
      "0.543 : 'men'\n",
      "0.539 : 'take'\n",
      "0.533 : 'walk'\n",
      "0.531 : 'knees'\n",
      "0.528 : 'right'\n",
      "0.527 : 'out'\n",
      "0.525 : 'full'\n",
      "0.523 : 'go'\n",
      "0.523 : 'fit'\n",
      "0.523 : 'quadruple'\n",
      "0.521 : 'under'\n",
      "0.519 : 'forced'\n",
      "0.517 : 'pair'\n",
      "0.516 : 'boots'\n",
      "0.515 : 'fight'\n",
      "0.513 : 'put'\n",
      "0.511 : 'leg'\n",
      "0.510 : 'knee'\n",
      "0.509 : 'get'\n",
      "0.508 : 'legs'\n",
      "\n",
      "None\n",
      "----------------------------------------------------\n",
      "Reverse order:\n",
      "'reptile' is to 'lizard' as 'mammal' is to ___\n",
      "0.831 : 'lizard'\n",
      "0.736 : 'mammal'\n",
      "0.651 : 'mammals'\n",
      "0.631 : 'species'\n",
      "0.615 : 'parrot'\n",
      "0.613 : 'lizards'\n",
      "0.578 : 'extinct'\n",
      "0.557 : 'fishes'\n",
      "0.554 : 'reptiles'\n",
      "0.553 : 'frog'\n",
      "0.552 : 'snake'\n",
      "0.552 : 'genus'\n",
      "0.552 : 'whale'\n",
      "0.545 : 'subspecies'\n",
      "0.533 : 'tortoise'\n",
      "\n",
      "None\n",
      "'hand' is to 'finger' as 'foot' is to ___\n",
      "0.809 : 'foot'\n",
      "0.693 : 'finger'\n",
      "0.640 : 'rib'\n",
      "0.627 : 'elbow'\n",
      "0.603 : 'metatarsal'\n",
      "0.599 : 'thumb'\n",
      "0.596 : 'forearm'\n",
      "0.586 : 'thigh'\n",
      "0.577 : 'toes'\n",
      "0.574 : 'calf'\n",
      "0.573 : 'groin'\n",
      "0.571 : 'bruise'\n",
      "0.567 : 'wrist'\n",
      "0.565 : 'ankle'\n",
      "0.562 : 'toe'\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (b).3\n",
    "print (show_analogy(hands, \"lizard\", \"reptile\", \"dog\", k=30))\n",
    "print (show_analogy(hands, \"finger\", \"hand\", \"toe\", k=30))\n",
    "print (\"----------------------------------------------------\")\n",
    "print (\"Reverse order:\")\n",
    "print (show_analogy(hands,  \"reptile\",\"lizard\", \"mammal\", k=15))\n",
    "print (show_analogy(hands, \"hand\", \"finger\", \"foot\", k=15))\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ans. The first correct answer should be `'dog'` to `'mammal'`, the second correct answer should be `'toe'` to `'foot'`, the model failed to capture it within the top 30 candidates. This is testing the relationship from hypohyms to hypernyms and our approach does not work well. Asking the question in the reverse question, the model does a poor job as well.\n",
    "\n",
    "The core assumption with linear analogies is that they map one to one linear relationship in vector space, essentially we are drawing single vector lines from one point (word) to anther.. `man` points to `woman` as `boy` points to `girl` and the reverse -- `woman` points to `man` as `girl` points to `boy` should be true as well. However, with hyponyms and hypernyms, the relationship mapped is many-to-one and one-to-many which cannot be mapped as single vector lines in vector space. When `reptile` map to `lizard`, `mammal` can map to `dog`, `fox`, or `pig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
